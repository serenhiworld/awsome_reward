#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‹±å›½ä¼˜æƒ æ¨èç ç½‘ç«™ - å…¨è‡ªåŠ¨åŒ–è¿è¡Œè„šæœ¬
è‡ªåŠ¨æ‰§è¡Œï¼šçˆ¬è™« â†’ ç¿»è¯‘ â†’ æ›´æ–°ç½‘ç«™ â†’ ç”ŸæˆæŠ¥å‘Š
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from pathlib import Path

# æ·»åŠ crawlerç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'crawler'))

try:
    from crawler.enhanced_crawler import EnhancedFreeStuffCrawler
except ImportError:
    # å¦‚æœè·¯å¾„æœ‰é—®é¢˜ï¼Œå°è¯•ç›´æ¥å¯¼å…¥
    from enhanced_crawler import EnhancedFreeStuffCrawler

from deal_renderer import (
    REQUIRED_REAL_DEALS,
    render_deals_section,
    replace_deals_section,
    select_real_deals,
)

class AutomationManager:
    """è‡ªåŠ¨åŒ–ç®¡ç†å™¨"""
    
    def __init__(self):
        self.setup_logging()
        self.project_root = Path(__file__).parent
        self.crawler_dir = self.project_root / 'crawler'
        self.data_dir = self.crawler_dir / 'data'
        self.display_deal_count = 0
        self.last_total_real_deals = 0
        self.required_real_deals = REQUIRED_REAL_DEALS
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('automation.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def run_crawler(self):
        """è¿è¡Œçˆ¬è™«"""
        self.logger.info("ğŸ¤– å¯åŠ¨çˆ¬è™«ç³»ç»Ÿ...")
        
        try:
            # åˆ‡æ¢åˆ°crawlerç›®å½•
            original_cwd = os.getcwd()
            os.chdir(self.crawler_dir)
            
            # è¿è¡Œçˆ¬è™«
            crawler = EnhancedFreeStuffCrawler()
            deals = crawler.run_crawler()
            
            os.chdir(original_cwd)
            
            if deals:
                self.logger.info(f"âœ… çˆ¬è™«æˆåŠŸè·å– {len(deals)} ä¸ªä¼˜æƒ ")
                return deals
            else:
                self.logger.warning("âš ï¸ çˆ¬è™«æœªè·å–åˆ°ä¼˜æƒ æ•°æ®")
                return []
                
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            return []
    
    def update_website(self, deals_data=None):
        """æ›´æ–°ç½‘ç«™å†…å®¹ï¼Œä»…å±•ç¤ºçœŸå®ä¼˜æƒ é“¾æ¥"""
        self.display_deal_count = 0
        if not deals_data:
            deals_data = self.get_latest_deals()

        if not deals_data:
            self.logger.error("âŒ æœªæ‰¾åˆ°å¯ç”¨äºæ›´æ–°çš„ç½‘ç«™ä¼˜æƒ æ•°æ®")
            return False

        display_deals, meets_requirement, total_real = select_real_deals(
            deals_data,
            required_count=self.required_real_deals,
        )

        self.last_total_real_deals = total_real

        if not meets_requirement or not display_deals:
            self.logger.error(
                "âŒ å®æ—¶æ•°æ®ä»…æ‰¾åˆ° %d æ¡çœŸå®ä¼˜æƒ ï¼Œæœªè¾¾åˆ° %d æ¡å±•ç¤ºè¦æ±‚",
                total_real,
                self.required_real_deals,
            )
            return False

        try:
            self.logger.info("ğŸŒ æ›´æ–°ç½‘ç«™å†…å®¹...")
            deals_section_html, metadata = render_deals_section(display_deals)

            index_file = self.project_root / 'index.html'
            with open(index_file, 'r', encoding='utf-8') as file:
                original_html = file.read()

            updated_html, action = replace_deals_section(original_html, deals_section_html)

            with open(index_file, 'w', encoding='utf-8') as file:
                file.write(updated_html)

            self.display_deal_count = len(display_deals)

            action_text = {
                'replaced': 'è¦†ç›–åŸæœ‰åŒºå—',
                'inserted': 'æ–°å¢æ¯æ—¥ä¼˜æƒ åŒºå—',
                'appended': 'è¿½åŠ æ¯æ—¥ä¼˜æƒ åŒºå—',
            }.get(action, action)

            self.logger.info("âœ… ç½‘ç«™å†…å®¹æ›´æ–°æˆåŠŸï¼ˆ%sï¼‰", action_text)
            self.logger.info(metadata['update_text'])
            return True

        except Exception as e:
            self.logger.error(f"âŒ ç½‘ç«™æ›´æ–°å¤±è´¥: {e}")
            return False
    
    def get_latest_deals(self):
        """è·å–æœ€æ–°çš„ä¼˜æƒ æ•°æ®"""
        try:
            # æŸ¥æ‰¾æœ€æ–°çš„enhanced_dealsæ–‡ä»¶
            json_files = list(self.data_dir.glob('enhanced_deals_*.json'))
            if not json_files:
                return []

            latest_file = max(json_files, key=lambda x: x.stat().st_mtime)

            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)

        except Exception as e:
            self.logger.error(f"è·å–æœ€æ–°æ•°æ®å¤±è´¥: {e}")
            return []
    
    def generate_report(self, deals_count=0, update_success=False):
        """ç”Ÿæˆè¿è¡ŒæŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        crawler_status = (
            'âœ… æˆåŠŸ'
            if self.last_total_real_deals >= self.required_real_deals
            else 'âŒ æœªè¾¾æ ‡'
        )
        website_status = 'âœ… æˆåŠŸ' if update_success else 'âŒ æœªæ›´æ–°'

        if self.last_total_real_deals == 0:
            suggestion = 'âŒ æœªæŠ“å–åˆ°çœŸå®ä¼˜æƒ ï¼Œè¯·æ£€æŸ¥çˆ¬è™«æˆ–æ•°æ®æº'
        elif self.last_total_real_deals < self.required_real_deals:
            suggestion = (
                f"âš ï¸ çœŸå®ä¼˜æƒ ä»… {self.last_total_real_deals} æ¡ï¼Œæœªè¾¾åˆ° "
                f"{self.required_real_deals} æ¡å±•ç¤ºè¦æ±‚"
            )
        elif not update_success:
            suggestion = 'âš ï¸ ç½‘ç«™æœªæ›´æ–°ï¼Œè¯·æ£€æŸ¥ HTML æ¨¡æ¿æˆ–å†™å…¥æƒé™'
        else:
            suggestion = 'âœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œç»§ç»­ä¿æŒæ¯æ—¥æ›´æ–°'

        report = f"""# ğŸ¤– è‡ªåŠ¨åŒ–è¿è¡ŒæŠ¥å‘Š

## ğŸ“… è¿è¡Œæ—¶é—´: {timestamp}

### âœ… è¿è¡Œç»“æœ

- **çˆ¬è™«çŠ¶æ€**: {crawler_status}
- **è·å–çœŸå®ä¼˜æƒ æ•°é‡**: {self.last_total_real_deals} æ¡
- **å±•ç¤ºä¼˜æƒ æ•°é‡**: {deals_count} æ¡
- **ç½‘ç«™æ›´æ–°**: {website_status}
- **çœŸå®é“¾æ¥æå–**: âœ… å·²å¯ç”¨

### ğŸ“Š ç³»ç»ŸçŠ¶æ€

- **çˆ¬è™«ç³»ç»Ÿ**: âœ… å·²æ‰§è¡Œ
- **ç¿»è¯‘åŠŸèƒ½**: âœ… æ­£å¸¸å·¥ä½œ
- **ç½‘ç«™æ›´æ–°**: {'âœ… è‡ªåŠ¨å®Œæˆ' if update_success else 'âš ï¸ éœ€äººå·¥æ£€æŸ¥'}
- **æ•°æ®å­˜å‚¨**: âœ… JSON + HTML

### ğŸŒ è®¿é—®ä¿¡æ¯

- **æœ¬åœ°é¢„è§ˆ**: http://localhost:8000
- **GitHub Pages**: éœ€è¦æ¨é€æ›´æ–°

### ğŸ“ ä¸‹æ¬¡è¿è¡Œå»ºè®®

{suggestion}

---
ğŸ‰ è‡ªåŠ¨åŒ–ç³»ç»Ÿè¿è¡Œå®Œæˆï¼
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.project_root / f'automation_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report
    
    def run_full_automation(self):
        """è¿è¡Œå®Œæ•´çš„è‡ªåŠ¨åŒ–æµç¨‹"""
        self.logger.info("ğŸš€ å¯åŠ¨å…¨è‡ªåŠ¨åŒ–æµç¨‹...")
        
        start_time = time.time()
        
        # 1. è¿è¡Œçˆ¬è™«
        deals = self.run_crawler()
        deals_count = len(deals)

        # 2. æ›´æ–°ç½‘ç«™
        update_success = self.update_website(deals)
        if not update_success:
            self.logger.error("ç½‘ç«™æ›´æ–°å¤±è´¥")

        display_count = self.display_deal_count if self.display_deal_count else deals_count

        # 3. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(display_count, update_success=update_success)
        
        end_time = time.time()
        duration = round(end_time - start_time, 2)
        
        self.logger.info(f"ğŸ‰ å…¨è‡ªåŠ¨åŒ–æµç¨‹å®Œæˆï¼è€—æ—¶: {duration}ç§’")
        
        # æ˜¾ç¤ºæ‘˜è¦
        print(f"\n{'='*60}")
        print("ğŸŠ è‹±å›½ä¼˜æƒ æ¨èç ç½‘ç«™ - è‡ªåŠ¨åŒ–å®Œæˆ")
        print(f"{'='*60}")
        print(f"ğŸ“Š å±•ç¤ºä¼˜æƒ : {display_count} ä¸ª")
        if not update_success:
            print(
                f"âŒ æœ¬æ¬¡æœªèƒ½å‘å¸ƒæ¯æ—¥ä¼˜æƒ ï¼ŒçœŸå®ä¼˜æƒ ä»… {self.last_total_real_deals} æ¡ï¼Œ"
                f"éœ€è¦è‡³å°‘ {self.required_real_deals} æ¡"
            )
        print(f"â±ï¸  è¿è¡Œæ—¶é—´: {duration} ç§’")
        print(f"ğŸŒ æœ¬åœ°é¢„è§ˆ: http://localhost:8000")
        print(f"{'='*60}")
        
        return update_success

def main():
    """ä¸»å‡½æ•°"""
    automation = AutomationManager()
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == 'crawler':
            # åªè¿è¡Œçˆ¬è™«
            deals = automation.run_crawler()
            print(f"çˆ¬è™«å®Œæˆï¼Œè·å– {len(deals)} ä¸ªä¼˜æƒ ")
            
        elif command == 'update':
            # åªæ›´æ–°ç½‘ç«™
            success = automation.update_website()
            print("ç½‘ç«™æ›´æ–°" + ("æˆåŠŸ" if success else "å¤±è´¥"))
            
        elif command == 'report':
            # åªç”ŸæˆæŠ¥å‘Š
            report = automation.generate_report()
            print("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
        else:
            print("æœªçŸ¥å‘½ä»¤ã€‚ä½¿ç”¨: python automation.py [crawler|update|report]")
            
    else:
        # è¿è¡Œå®Œæ•´æµç¨‹
        automation.run_full_automation()

if __name__ == "__main__":
    main()
