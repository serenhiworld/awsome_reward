#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‹±å›½ä¼˜æƒ æ¨èç ç½‘ç«™ - å…¨è‡ªåŠ¨åŒ–è¿è¡Œè„šæœ¬
è‡ªåŠ¨æ‰§è¡Œï¼šçˆ¬è™« â†’ ç¿»è¯‘ â†’ æ›´æ–°ç½‘ç«™ â†’ ç”ŸæˆæŠ¥å‘Š
"""

import os
import sys
import json
import time
import logging
import textwrap
from datetime import datetime
from pathlib import Path

# æ·»åŠ crawlerç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'crawler'))

try:
    from crawler.enhanced_crawler import EnhancedFreeStuffCrawler
except ImportError:
    # å¦‚æœè·¯å¾„æœ‰é—®é¢˜ï¼Œå°è¯•ç›´æ¥å¯¼å…¥
    from enhanced_crawler import EnhancedFreeStuffCrawler

from bs4 import BeautifulSoup

class AutomationManager:
    """è‡ªåŠ¨åŒ–ç®¡ç†å™¨"""
    
    def __init__(self):
        self.setup_logging()
        self.project_root = Path(__file__).parent
        self.crawler_dir = self.project_root / 'crawler'
        self.data_dir = self.crawler_dir / 'data'
        self.sample_data_dir = self.crawler_dir / 'sample_data'
        self.sample_data_file = self.sample_data_dir / 'enhanced_deals_sample.json'
        self.last_update_used_fallback = False
        self.display_deal_count = 0
        self.required_real_deals = 6
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('automation.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def run_crawler(self):
        """è¿è¡Œçˆ¬è™«"""
        self.logger.info("ğŸ¤– å¯åŠ¨çˆ¬è™«ç³»ç»Ÿ...")
        
        try:
            # åˆ‡æ¢åˆ°crawlerç›®å½•
            original_cwd = os.getcwd()
            os.chdir(self.crawler_dir)
            
            # è¿è¡Œçˆ¬è™«
            crawler = EnhancedFreeStuffCrawler()
            deals = crawler.run_crawler()
            
            os.chdir(original_cwd)
            
            if deals:
                self.logger.info(f"âœ… çˆ¬è™«æˆåŠŸè·å– {len(deals)} ä¸ªä¼˜æƒ ")
                return deals
            else:
                self.logger.warning("âš ï¸ çˆ¬è™«æœªè·å–åˆ°ä¼˜æƒ æ•°æ®")
                return []
                
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            return []
    
    def load_fallback_deals(self):
        """åŠ è½½ç¤ºä¾‹ä¼˜æƒ æ•°æ®ä½œä¸ºå…œåº•"""
        if self.sample_data_file.exists():
            try:
                with open(self.sample_data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if data:
                    self.logger.warning("âš ï¸ æœªè·å–åˆ°å®æ—¶ä¼˜æƒ ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œå±•ç¤º")
                    return data
            except Exception as exc:
                self.logger.error(f"è¯»å–ç¤ºä¾‹ä¼˜æƒ æ•°æ®å¤±è´¥: {exc}")
        else:
            self.logger.error("æœªæ‰¾åˆ°ç¤ºä¾‹ä¼˜æƒ æ•°æ®æ–‡ä»¶ crawler/sample_data/enhanced_deals_sample.json")
        return []

    def is_real_deal_link(self, url):
        """åˆ¤æ–­æ˜¯å¦ä¸ºçœŸå®å¯ç”¨çš„å¤–éƒ¨ä¼˜æƒ é“¾æ¥"""
        if not url or not isinstance(url, str):
            return False

        url = url.strip()
        if not url.lower().startswith("http"):
            return False

        blocked_domains = [
            "latestfreestuff.co.uk",
            "facebook.com",
            "twitter.com",
            "instagram.com",
            "youtube.com"
        ]

        return all(domain not in url for domain in blocked_domains)

    def prepare_display_deals(self, deals, required_count=None):
        """ç­›é€‰å‡ºçœŸå®çš„ä¼˜æƒ é“¾æ¥ï¼Œå¹¶ç¡®ä¿æ•°é‡æ»¡è¶³å±•ç¤ºéœ€æ±‚"""
        if required_count is None:
            required_count = self.required_real_deals

        real_deals = []

        for deal in deals or []:
            if not isinstance(deal, dict):
                continue

            url = deal.get('url') or deal.get('claim_url') or deal.get('source_url')
            if not self.is_real_deal_link(url):
                continue

            normalized = deal.copy()
            normalized['url'] = url.strip()
            real_deals.append(normalized)

        meets_requirement = len(real_deals) >= required_count
        return real_deals[:required_count], meets_requirement

    def update_website(self, deals_data=None):
        """æ›´æ–°ç½‘ç«™å†…å®¹"""
        self.last_update_used_fallback = False
        if not deals_data:
            # è·å–æœ€æ–°çš„æ•°æ®æ–‡ä»¶
            deals_data = self.get_latest_deals()

        if not deals_data:
            deals_data = self.load_fallback_deals()
            if not deals_data:
                self.logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„ä¼˜æƒ æ•°æ®æ¥æ›´æ–°ç½‘ç«™")
                return False
            self.last_update_used_fallback = True

        display_deals, has_enough_real = self.prepare_display_deals(deals_data)

        if not has_enough_real:
            self.logger.warning(
                "âš ï¸ å®æ—¶æ•°æ®ä¸­çœŸå®ä¼˜æƒ ä¸è¶³ %d æ¡ï¼Œå°è¯•ä½¿ç”¨ç¤ºä¾‹æ•°æ®", self.required_real_deals
            )
            fallback_deals = self.load_fallback_deals()
            display_deals, fallback_has_enough = self.prepare_display_deals(fallback_deals)

            if not display_deals:
                self.logger.error("âŒ æœªèƒ½å‡†å¤‡å‡ºç”¨äºå±•ç¤ºçš„çœŸå®ä¼˜æƒ å†…å®¹")
                return False

            self.last_update_used_fallback = not fallback_has_enough
        else:
            self.last_update_used_fallback = False

        self.display_deal_count = len(display_deals)

        try:
            self.logger.info("ğŸŒ æ›´æ–°ç½‘ç«™å†…å®¹...")

            # ç”Ÿæˆæ–°çš„HTMLå†…å®¹
            deals_content = self.generate_deals_html(display_deals, used_fallback=self.last_update_used_fallback)

            # æ›´æ–°index.htmlä¸­çš„ä¼˜æƒ éƒ¨åˆ†
            self.update_index_html(deals_content)
            
            self.logger.info("âœ… ç½‘ç«™å†…å®¹æ›´æ–°æˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç½‘ç«™æ›´æ–°å¤±è´¥: {e}")
            return False
    
    def get_latest_deals(self):
        """è·å–æœ€æ–°çš„ä¼˜æƒ æ•°æ®"""
        try:
            # æŸ¥æ‰¾æœ€æ–°çš„enhanced_dealsæ–‡ä»¶
            json_files = list(self.data_dir.glob('enhanced_deals_*.json'))
            if not json_files:
                return []

            latest_file = max(json_files, key=lambda x: x.stat().st_mtime)

            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)

        except Exception as e:
            self.logger.error(f"è·å–æœ€æ–°æ•°æ®å¤±è´¥: {e}")
            return []
    
    def generate_deals_html(self, deals, used_fallback=False):
        """ç”Ÿæˆä¼˜æƒ HTMLå†…å®¹ï¼Œå¹¶è¿”å›æ›´æ–°æ‰€éœ€çš„ç»„ä»¶"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        deals_count = len(deals)
        header_text = f"ğŸ ä»Šæ—¥è‹±å›½ä¼˜æƒ ç²¾é€‰ - {deals_count} ä¸ªçœŸå®å•†å®¶ä¼˜æƒ "
        if used_fallback:
            header_text += "ï¼ˆç¤ºä¾‹æ•°æ®ï¼‰"

        update_text = f"ğŸ•’ æœ€æ–°æ›´æ–°: {timestamp}"
        if used_fallback:
            update_text += " | âš ï¸ æš‚æ— å®æ—¶æ•°æ®ï¼Œå±•ç¤ºç¤ºä¾‹ä¼˜æƒ "
        else:
            update_text += " | âœ… æå–çœŸå®ä¼˜æƒ é“¾æ¥"

        deal_items = []
        for deal in deals:
            title = deal.get('title_zh', deal.get('title', '')).strip()
            description = deal.get('description_zh', deal.get('description', '')).strip()
            if len(description) > 150:
                description = description[:150].rstrip() + "..."

            url = deal.get('url', '#')
            source_url = deal.get('source_url', deal.get('detail_url', '#'))
            date = deal.get('date', '')
            image = deal.get('image', '')

            is_real_link = bool(url and 'latestfreestuff.co.uk' not in url)

            try:
                from urllib.parse import urlparse
                domain = urlparse(url).netloc if url.startswith('http') else 'æœªçŸ¥åŸŸå'
            except Exception:
                domain = 'æœªçŸ¥åŸŸå'

            item_class = "deal-item featured-deal" if is_real_link else "deal-item"

            item_html = f"""
<div class=\"{item_class}\">"""

            if is_real_link:
                item_html += """
    <div class=\"deal-badge\">âœ… çœŸå®é“¾æ¥</div>"""

            if image:
                item_html += f"""
    <div class=\"deal-image\">
        <img src=\"{image}\" alt=\"ä¼˜æƒ å›¾ç‰‡\" loading=\"lazy\">
    </div>"""

            item_html += f"""
    <h3>{title}</h3>
    <p>{description}</p>
    <div class=\"deal-meta\">
        <span class=\"date\">ğŸ“… {date}</span>
        <span class=\"domain\">ğŸŒ {domain}</span>"""

            if is_real_link:
                item_html += f"""
        <a href=\"{url}\" target=\"_blank\" class=\"deal-link btn-primary\">ğŸ ç«‹å³é¢†å–</a>"""
            else:
                item_html += f"""
        <a href=\"{source_url}\" target=\"_blank\" class=\"deal-link\">æŸ¥çœ‹è¯¦æƒ…</a>"""

            item_html += """
    </div>
</div>"""

            deal_items.append(textwrap.indent(item_html.strip(), " " * 20))

        if not deal_items:
            deal_items.append(" " * 20 + "<div class=\"deal-item\">æš‚æ— æœ€æ–°ä¼˜æƒ ï¼Œæ•¬è¯·å…³æ³¨ï¼</div>")

        deals_container = "\n".join([
            " " * 16 + '<div class="deals-container">',
            "\n".join(deal_items),
            " " * 16 + '</div>'
        ])

        return {
            "header_text": header_text,
            "update_text": update_text,
            "container_html": deals_container
        }

    def update_index_html(self, deals_content):
        """æ›´æ–°index.htmlä¸­çš„ä¼˜æƒ éƒ¨åˆ†"""
        try:
            index_file = self.project_root / 'index.html'

            # è¯»å–ç°æœ‰å†…å®¹
            with open(index_file, 'r', encoding='utf-8') as f:
                content = f.read()

            soup = BeautifulSoup(content, 'html.parser')

            deals_section = soup.find('section', {'id': 'deals', 'class': 'daily-deals'})
            container = None
            wrapper = None

            if not deals_section:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ° id ä¸º deals çš„æ¯æ—¥ä¼˜æƒ åŒºå—ï¼Œè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªæ–°çš„åŒºå—")
                deals_section = soup.new_tag('section', attrs={'id': 'deals', 'class': 'daily-deals'})
                container = soup.new_tag('div', attrs={'class': 'container'})
                wrapper = soup.new_tag('div', attrs={'class': 'daily-deals-section'})
                container.append(wrapper)
                deals_section.append(container)

                benefits_section = soup.find('section', {'id': 'benefits'})
                if benefits_section:
                    benefits_section.insert_before(deals_section)
                else:
                    body = soup.body or soup
                    body.append(deals_section)
            else:
                container = deals_section.find('div', class_='container')
                if not container:
                    container = soup.new_tag('div', attrs={'class': 'container'})
                    deals_section.append(container)

                wrapper = container.find('div', class_='daily-deals-section')

            if not wrapper:
                wrapper = soup.new_tag('div', attrs={'class': 'daily-deals-section'})
                container.append(wrapper)

            header = wrapper.find('h2')
            if not header:
                header = soup.new_tag('h2')
                wrapper.insert(0, header)
            header.string = deals_content['header_text']

            update_time = wrapper.find('p', class_='update-time')
            if not update_time:
                update_time = soup.new_tag('p', attrs={'class': 'update-time'})
                header.insert_after(update_time)
            update_time.string = deals_content['update_text']

            new_container_soup = BeautifulSoup(deals_content['container_html'], 'html.parser')
            new_container = new_container_soup.find('div', class_='deals-container') or new_container_soup
            if new_container.has_attr('class'):
                new_container['class'] = [cls for cls in new_container.get('class', []) if cls != 'placeholder']

            # åˆ é™¤å ä½ç¬¦å…ƒç´ 
            for placeholder in wrapper.select('.placeholder, .placeholder-message'):
                placeholder.decompose()

            existing_container = wrapper.find('div', class_='deals-container')
            if existing_container:
                existing_container.replace_with(new_container)
            else:
                wrapper.append(new_container)

            with open(index_file, 'w', encoding='utf-8') as f:
                f.write(str(soup))

            return True

        except Exception as e:
            self.logger.error(f"æ›´æ–°index.htmlå¤±è´¥: {e}")
            return False
    
    def generate_report(self, deals_count=0, used_fallback=False):
        """ç”Ÿæˆè¿è¡ŒæŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        crawler_status = 'âœ… æˆåŠŸ' if deals_count > 0 else ('âš ï¸ ä½¿ç”¨ç¤ºä¾‹æ•°æ®' if used_fallback else 'âŒ å¤±è´¥')
        website_status = 'âœ… æˆåŠŸ' if deals_count > 0 or used_fallback else 'âš ï¸ è·³è¿‡'
        suggestion = (
            'âš ï¸ éœ€è¦æ£€æŸ¥çˆ¬è™«è®¾ç½®æˆ–ç½‘ç«™çŠ¶æ€'
            if deals_count == 0 and not used_fallback
            else 'âš ï¸ ä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼Œè¯·æ£€æŸ¥çˆ¬è™«çŠ¶æ€'
            if used_fallback
            else 'âœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œç»§ç»­å®šæ—¶æ‰§è¡Œ'
        )

        report = f"""# ğŸ¤– è‡ªåŠ¨åŒ–è¿è¡ŒæŠ¥å‘Š

## ğŸ“… è¿è¡Œæ—¶é—´: {timestamp}

### âœ… è¿è¡Œç»“æœ

- **çˆ¬è™«çŠ¶æ€**: {crawler_status}
- **è·å–ä¼˜æƒ æ•°é‡**: {deals_count} ä¸ª
- **ç½‘ç«™æ›´æ–°**: {website_status}
- **çœŸå®é“¾æ¥æå–**: âœ… å·²å¯ç”¨

### ğŸ“Š ç³»ç»ŸçŠ¶æ€

- **çˆ¬è™«ç³»ç»Ÿ**: âœ… æ­£å¸¸è¿è¡Œ
- **ç¿»è¯‘åŠŸèƒ½**: âœ… æ­£å¸¸å·¥ä½œ
- **ç½‘ç«™æ›´æ–°**: âœ… è‡ªåŠ¨å®Œæˆ
- **æ•°æ®å­˜å‚¨**: âœ… JSON + HTML

### ğŸŒ è®¿é—®ä¿¡æ¯

- **æœ¬åœ°é¢„è§ˆ**: http://localhost:8000
- **GitHub Pages**: éœ€è¦æ¨é€æ›´æ–°

### ğŸ“ ä¸‹æ¬¡è¿è¡Œå»ºè®®

{suggestion}

---
ğŸ‰ è‡ªåŠ¨åŒ–ç³»ç»Ÿè¿è¡Œå®Œæˆï¼
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.project_root / f'automation_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report
    
    def run_full_automation(self):
        """è¿è¡Œå®Œæ•´çš„è‡ªåŠ¨åŒ–æµç¨‹"""
        self.logger.info("ğŸš€ å¯åŠ¨å…¨è‡ªåŠ¨åŒ–æµç¨‹...")
        
        start_time = time.time()
        
        # 1. è¿è¡Œçˆ¬è™«
        deals = self.run_crawler()
        deals_count = len(deals)

        # 2. æ›´æ–°ç½‘ç«™
        update_success = self.update_website(deals)
        if not update_success:
            self.logger.error("ç½‘ç«™æ›´æ–°å¤±è´¥")

        display_count = self.display_deal_count if self.display_deal_count else deals_count

        # 3. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(display_count, used_fallback=self.last_update_used_fallback)
        
        end_time = time.time()
        duration = round(end_time - start_time, 2)
        
        self.logger.info(f"ğŸ‰ å…¨è‡ªåŠ¨åŒ–æµç¨‹å®Œæˆï¼è€—æ—¶: {duration}ç§’")
        
        # æ˜¾ç¤ºæ‘˜è¦
        print(f"\n{'='*60}")
        print(f"ğŸŠ è‹±å›½ä¼˜æƒ æ¨èç ç½‘ç«™ - è‡ªåŠ¨åŒ–å®Œæˆ")
        print(f"{'='*60}")
        print(f"ğŸ“Š å±•ç¤ºä¼˜æƒ : {display_count} ä¸ª")
        if self.last_update_used_fallback and deals_count == 0:
            print("âš ï¸ æœ¬æ¬¡å±•ç¤ºç¤ºä¾‹ä¼˜æƒ æ•°æ®ï¼Œè¯·æ£€æŸ¥çˆ¬è™«æˆ–ç½‘ç»œè¿æ¥")
        print(f"â±ï¸  è¿è¡Œæ—¶é—´: {duration} ç§’")
        print(f"ğŸŒ æœ¬åœ°é¢„è§ˆ: http://localhost:8000")
        print(f"{'='*60}")
        
        return update_success

def main():
    """ä¸»å‡½æ•°"""
    automation = AutomationManager()
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == 'crawler':
            # åªè¿è¡Œçˆ¬è™«
            deals = automation.run_crawler()
            print(f"çˆ¬è™«å®Œæˆï¼Œè·å– {len(deals)} ä¸ªä¼˜æƒ ")
            
        elif command == 'update':
            # åªæ›´æ–°ç½‘ç«™
            success = automation.update_website()
            print("ç½‘ç«™æ›´æ–°" + ("æˆåŠŸ" if success else "å¤±è´¥"))
            
        elif command == 'report':
            # åªç”ŸæˆæŠ¥å‘Š
            report = automation.generate_report()
            print("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
        else:
            print("æœªçŸ¥å‘½ä»¤ã€‚ä½¿ç”¨: python automation.py [crawler|update|report]")
            
    else:
        # è¿è¡Œå®Œæ•´æµç¨‹
        automation.run_full_automation()

if __name__ == "__main__":
    main()
