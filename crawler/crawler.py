import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime, timedelta
from googletrans import Translator
import re
import os
from fake_useragent import UserAgent
import logging

class LatestFreeStuffCrawler:
    def __init__(self):
        self.base_url = "https://www.latestfreestuff.co.uk"
        self.ua = UserAgent()
        self.translator = Translator()
        self.session = requests.Session()
        self.setup_logging()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        self.session.headers.update(self.headers)
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def get_page_content(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«å°
            time.sleep(2)
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.content
        except Exception as e:
            self.logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None

    def parse_deals(self, html_content):
        """è§£æä¼˜æƒ ä¿¡æ¯"""
        if not html_content:
            return []
            
        soup = BeautifulSoup(html_content, 'lxml')
        deals = []
        
        # å¯»æ‰¾æ–‡ç« æˆ–ä¼˜æƒ ä¿¡æ¯çš„å®¹å™¨
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…ç½‘ç«™ç»“æ„è°ƒæ•´é€‰æ‹©å™¨
        deal_containers = soup.find_all('article') or soup.find_all('div', class_=re.compile(r'deal|post|item'))
        
        for container in deal_containers[:10]:  # é™åˆ¶æœ€å¤š10ä¸ªä¼˜æƒ 
            try:
                deal = self.extract_deal_info(container)
                if deal and self.is_valid_deal(deal):
                    deals.append(deal)
            except Exception as e:
                self.logger.error(f"è§£æä¼˜æƒ ä¿¡æ¯å¤±è´¥: {e}")
                continue
                
        return deals

    def extract_deal_info(self, container):
        """ä»å®¹å™¨ä¸­æå–ä¼˜æƒ ä¿¡æ¯"""
        deal = {}
        
        # æå–æ ‡é¢˜
        title_elem = (container.find('h1') or 
                     container.find('h2') or 
                     container.find('h3') or
                     container.find('a'))
        if title_elem:
            deal['title'] = title_elem.get_text().strip()
        else:
            return None
            
        # æå–æè¿°
        desc_elem = (container.find('p') or 
                    container.find('div', class_=re.compile(r'content|desc|summary')))
        if desc_elem:
            deal['description'] = desc_elem.get_text().strip()[:500]  # é™åˆ¶é•¿åº¦
        else:
            deal['description'] = ""
            
        # æå–é“¾æ¥
        link_elem = container.find('a')
        if link_elem:
            href = link_elem.get('href')
            if href:
                if href.startswith('/'):
                    deal['url'] = self.base_url + href
                elif not href.startswith('http'):
                    deal['url'] = self.base_url + '/' + href
                else:
                    deal['url'] = href
                    
        # æå–å›¾ç‰‡
        img_elem = container.find('img')
        if img_elem:
            src = img_elem.get('src') or img_elem.get('data-src')
            if src:
                if src.startswith('/'):
                    deal['image'] = self.base_url + src
                elif not src.startswith('http'):
                    deal['image'] = self.base_url + '/' + src
                else:
                    deal['image'] = src
                    
        # æå–æ—¥æœŸ
        date_elem = container.find('time') or container.find(attrs={'class': re.compile(r'date|time')})
        if date_elem:
            deal['date'] = date_elem.get_text().strip()
        else:
            deal['date'] = datetime.now().strftime('%Y-%m-%d')
            
        return deal

    def is_valid_deal(self, deal):
        """éªŒè¯ä¼˜æƒ ä¿¡æ¯æ˜¯å¦æœ‰æ•ˆ"""
        if not deal.get('title'):
            return False
        if len(deal['title']) < 10:  # æ ‡é¢˜å¤ªçŸ­
            return False
        if not deal.get('url'):
            return False
        return True

    def translate_text(self, text, max_retries=3):
        """ç¿»è¯‘æ–‡æœ¬åˆ°ä¸­æ–‡"""
        if not text:
            return ""
            
        for attempt in range(max_retries):
            try:
                # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…ç¿»è¯‘APIé™åˆ¶
                if len(text) > 1000:
                    text = text[:1000] + "..."
                    
                result = self.translator.translate(text, dest='zh-cn')
                return result.text
            except Exception as e:
                self.logger.warning(f"ç¿»è¯‘å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    return text  # ç¿»è¯‘å¤±è´¥è¿”å›åŸæ–‡

    def translate_deal(self, deal):
        """ç¿»è¯‘ä¼˜æƒ ä¿¡æ¯"""
        translated_deal = deal.copy()
        
        # ç¿»è¯‘æ ‡é¢˜
        translated_deal['title_zh'] = self.translate_text(deal['title'])
        
        # ç¿»è¯‘æè¿°
        if deal.get('description'):
            translated_deal['description_zh'] = self.translate_text(deal['description'])
        
        return translated_deal

    def save_deals_to_json(self, deals, filename=None):
        """ä¿å­˜ä¼˜æƒ ä¿¡æ¯åˆ°JSONæ–‡ä»¶"""
        if not filename:
            filename = f"deals_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
        filepath = os.path.join('data', filename)
        os.makedirs('data', exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(deals, f, ensure_ascii=False, indent=2)
            
        self.logger.info(f"å·²ä¿å­˜ {len(deals)} ä¸ªä¼˜æƒ ä¿¡æ¯åˆ° {filepath}")
        return filepath

    def generate_html_content(self, deals):
        """ç”ŸæˆHTMLå†…å®¹ç”¨äºç½‘ç«™å±•ç¤º"""
        html_template = """
        <div class="deals-container">
            <h2>ğŸ ä»Šæ—¥è‹±å›½ä¼˜æƒ ç²¾é€‰</h2>
            <p class="update-time">æ›´æ–°æ—¶é—´: {update_time}</p>
            <div class="deals-grid">
                {deals_html}
            </div>
        </div>
        """
        
        deal_template = """
        <div class="deal-card">
            {image_html}
            <div class="deal-content">
                <h3>{title}</h3>
                <p class="deal-description">{description}</p>
                <div class="deal-meta">
                    <span class="deal-date">ğŸ“… {date}</span>
                    <a href="{url}" target="_blank" class="deal-link">æŸ¥çœ‹è¯¦æƒ… â†’</a>
                </div>
            </div>
        </div>
        """
        
        deals_html = ""
        for deal in deals:
            image_html = ""
            if deal.get('image'):
                image_html = f'<img src="{deal["image"]}" alt="{deal["title_zh"]}" class="deal-image">'
                
            deal_html = deal_template.format(
                image_html=image_html,
                title=deal.get('title_zh', deal.get('title', '')),
                description=deal.get('description_zh', deal.get('description', ''))[:200] + "...",
                date=deal.get('date', ''),
                url=deal.get('url', '#')
            )
            deals_html += deal_html
            
        final_html = html_template.format(
            update_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            deals_html=deals_html
        )
        
        return final_html

    def crawl_and_translate(self):
        """ä¸»çˆ¬è™«æµç¨‹"""
        self.logger.info("å¼€å§‹çˆ¬å–æœ€æ–°ä¼˜æƒ ä¿¡æ¯...")
        
        # è·å–ä¸»é¡µå†…å®¹
        html_content = self.get_page_content(self.base_url)
        if not html_content:
            self.logger.error("æ— æ³•è·å–ç½‘ç«™å†…å®¹")
            return []
            
        # è§£æä¼˜æƒ ä¿¡æ¯
        deals = self.parse_deals(html_content)
        self.logger.info(f"æ‰¾åˆ° {len(deals)} ä¸ªä¼˜æƒ ä¿¡æ¯")
        
        if not deals:
            return []
            
        # ç¿»è¯‘ä¼˜æƒ ä¿¡æ¯
        translated_deals = []
        for i, deal in enumerate(deals):
            self.logger.info(f"ç¿»è¯‘ç¬¬ {i+1}/{len(deals)} ä¸ªä¼˜æƒ ...")
            translated_deal = self.translate_deal(deal)
            translated_deals.append(translated_deal)
            time.sleep(1)  # é¿å…ç¿»è¯‘APIé™åˆ¶
            
        # ä¿å­˜æ•°æ®
        json_file = self.save_deals_to_json(translated_deals)
        
        # ç”ŸæˆHTMLå†…å®¹
        html_content = self.generate_html_content(translated_deals)
        
        # ä¿å­˜HTMLæ–‡ä»¶
        html_file = os.path.join('data', f"deals_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html")
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
            
        self.logger.info(f"çˆ¬è™«å®Œæˆï¼ç”Ÿæˆæ–‡ä»¶: {json_file}, {html_file}")
        return translated_deals

def main():
    """ä¸»å‡½æ•°"""
    crawler = LatestFreeStuffCrawler()
    deals = crawler.crawl_and_translate()
    
    if deals:
        print(f"æˆåŠŸçˆ¬å–å¹¶ç¿»è¯‘äº† {len(deals)} ä¸ªä¼˜æƒ ä¿¡æ¯")
    else:
        print("æœªæ‰¾åˆ°ä¼˜æƒ ä¿¡æ¯")

if __name__ == "__main__":
    main()
