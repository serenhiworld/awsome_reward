import requests
import json
import re
import time
from datetime import datetime
from html.parser import HTMLParser
import logging
import os
from urllib.parse import urljoin, urlparse

# å¯¼å…¥é…ç½®
try:
    from enhanced_config import *
except ImportError:
    # å¦‚æœæ²¡æœ‰enhanced_configï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    BASE_URL = "https://www.latestfreestuff.co.uk"
    MAX_DEALS = 20
    REQUEST_DELAY = 2
    REQUEST_TIMEOUT = 30
    ENABLE_TRANSLATION = True

class SimpleTranslator:
    """ç®€å•çš„ç¿»è¯‘æœåŠ¡ï¼ˆå¯æ›¿æ¢ä¸ºå…¶ä»–ç¿»è¯‘APIï¼‰"""
    
    def __init__(self):
        self.cache = {}  # ç¿»è¯‘ç¼“å­˜
        
    def translate_to_chinese(self, text):
        """ç®€å•çš„è‹±è¯‘ä¸­ï¼ˆè¿™é‡Œä½¿ç”¨åŸºç¡€è¯æ±‡æ›¿æ¢ï¼Œå®é™…åº”ç”¨ä¸­å»ºè®®ä½¿ç”¨ä¸“ä¸šç¿»è¯‘APIï¼‰"""
        if not text or text in self.cache:
            return self.cache.get(text, text)
            
        # åŸºç¡€è¯æ±‡æ›¿æ¢ï¼ˆå¯æ‰©å±•ï¼‰
        translations = {
            'free': 'å…è´¹',
            'deal': 'ä¼˜æƒ ',
            'offer': 'ä¼˜æƒ ',
            'discount': 'æŠ˜æ‰£',
            'save': 'çœé’±',
            'sale': 'ä¿ƒé”€',
            'voucher': 'ä¼˜æƒ åˆ¸',
            'code': 'ä»£ç ',
            'cashback': 'è¿”ç°',
            'student': 'å­¦ç”Ÿ',
            'new': 'æ–°',
            'exclusive': 'ç‹¬å®¶',
            'limited': 'é™æ—¶',
            'today': 'ä»Šå¤©',
            'now': 'ç°åœ¨',
            'get': 'è·å¾—',
            'buy': 'è´­ä¹°',
            'shop': 'è´­ç‰©',
            'online': 'åœ¨çº¿',
            'delivery': 'é…é€',
            'shipping': 'è¿è´¹',
            'click': 'ç‚¹å‡»',
            'here': 'è¿™é‡Œ',
            'link': 'é“¾æ¥',
            'visit': 'è®¿é—®',
            'website': 'ç½‘ç«™',
            'store': 'å•†åº—',
            'price': 'ä»·æ ¼',
            'cheap': 'ä¾¿å®œ',
            'bargain': 'ä¾¿å®œè´§',
            'member': 'ä¼šå‘˜',
            'signup': 'æ³¨å†Œ',
            'register': 'æ³¨å†Œ',
            'account': 'è´¦æˆ·'
        }
        
        translated = text.lower()
        for en, zh in translations.items():
            translated = translated.replace(en, zh)
            
        # ä¿æŒåŸæ–‡çš„å¤§å°å†™ç»“æ„
        result = self._preserve_case_structure(text, translated)
        self.cache[text] = result
        return result
        
    def _preserve_case_structure(self, original, translated):
        """ä¿æŒåŸæ–‡çš„å¤§å°å†™ç»“æ„"""
        if not original:
            return translated
        if original.isupper():
            return translated.upper()
        if original.istitle():
            return translated.title()
        return translated

class DealParser(HTMLParser):
    """HTMLè§£æå™¨"""
    
    def __init__(self):
        super().__init__()
        self.deals = []
        self.current_deal = {}
        self.in_deal_container = False
        self.in_title = False
        self.in_description = False
        self.current_tag = None
        
    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        attrs_dict = dict(attrs)
        
        # æ£€æµ‹å¯èƒ½çš„ä¼˜æƒ å®¹å™¨
        if tag in ['article', 'div'] and any('deal' in str(v).lower() or 'post' in str(v).lower() 
                                           for v in attrs_dict.values()):
            self.in_deal_container = True
            self.current_deal = {}
            
        # æ£€æµ‹æ ‡é¢˜
        if tag in ['h1', 'h2', 'h3', 'h4'] and self.in_deal_container:
            self.in_title = True
            
        # æ£€æµ‹æè¿°
        if tag == 'p' and self.in_deal_container:
            self.in_description = True
            
        # æ£€æµ‹é“¾æ¥ - è·å–è¯¦æƒ…é¡µé“¾æ¥
        if tag == 'a' and self.in_deal_container and 'href' in attrs_dict:
            if 'detail_url' not in self.current_deal:
                self.current_deal['detail_url'] = attrs_dict['href']
                
        # æ£€æµ‹å›¾ç‰‡
        if tag == 'img' and self.in_deal_container:
            if 'src' in attrs_dict:
                self.current_deal['image'] = attrs_dict['src']
            elif 'data-src' in attrs_dict:
                self.current_deal['image'] = attrs_dict['data-src']
                
    def handle_endtag(self, tag):
        if tag in ['article', 'div'] and self.in_deal_container:
            if self.current_deal and 'title' in self.current_deal:
                self.deals.append(self.current_deal.copy())
            self.in_deal_container = False
            self.current_deal = {}
            
        if tag in ['h1', 'h2', 'h3', 'h4']:
            self.in_title = False
            
        if tag == 'p':
            self.in_description = False
            
    def handle_data(self, data):
        data = data.strip()
        if not data:
            return
            
        if self.in_title and self.in_deal_container:
            self.current_deal['title'] = data
            
        if self.in_description and self.in_deal_container:
            if 'description' not in self.current_deal:
                self.current_deal['description'] = data
            else:
                self.current_deal['description'] += ' ' + data

class EnhancedFreeStuffCrawler:
    """å¢å¼ºç‰ˆä¼˜æƒ çˆ¬è™« - è·å–çœŸå®ä¼˜æƒ é“¾æ¥"""
    
    def __init__(self):
        self.base_url = BASE_URL
        self.translator = SimpleTranslator()
        self.session = requests.Session()
        self.setup_logging()

        self.max_deals = MAX_DEALS if isinstance(MAX_DEALS, int) and MAX_DEALS > 0 else 20
        self.request_delay = REQUEST_DELAY if isinstance(REQUEST_DELAY, (int, float)) else 2
        self.request_timeout = REQUEST_TIMEOUT if isinstance(REQUEST_TIMEOUT, (int, float)) else 30
        self.enable_translation = bool(ENABLE_TRANSLATION)

        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.data_dir = os.path.join(self.base_dir, 'data')
        os.makedirs(self.data_dir, exist_ok=True)

        # è®¾ç½®è¯·æ±‚å¤´
        default_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }

        # å¦‚æœé…ç½®æ–‡ä»¶æä¾›è‡ªå®šä¹‰è¯·æ±‚å¤´åˆ™è¦†ç›–
        custom_headers = globals().get('HEADERS', {})
        default_headers.update(custom_headers if isinstance(custom_headers, dict) else {})
        self.headers = default_headers

        self.session.headers.update(self.headers)
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('enhanced_crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def get_page_content(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            self.logger.info(f"æ­£åœ¨è·å–é¡µé¢: {url}")
            response = self.session.get(url, timeout=self.request_timeout)
            response.raise_for_status()
            return response.text
        except Exception as e:
            self.logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None

    def ensure_absolute_url(self, url):
        """ç¡®ä¿URLä¸ºç»å¯¹åœ°å€"""
        if not url:
            return ''
        if url.startswith(('http://', 'https://')):
            return url
        return urljoin(self.base_url, url)

    def get_domain(self, url):
        """æå–åŸŸåå¹¶è¿›è¡Œæ ¼å¼åŒ–"""
        if not url:
            return ''
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        if domain.startswith('www.'):
            domain = domain[4:]
        return domain

    def verify_deal_link(self, url):
        """éªŒè¯ä¼˜æƒ é“¾æ¥æ˜¯å¦å¯è®¿é—®"""
        if not url:
            return False

        try:
            head_resp = self.session.head(url, allow_redirects=True, timeout=self.request_timeout)
            status_code = head_resp.status_code

            if status_code >= 400:
                self.logger.info(f"HEAD è¯·æ±‚è¿”å› {status_code}ï¼Œå°è¯• GET: {url}")
                get_resp = self.session.get(url, allow_redirects=True, timeout=self.request_timeout)
                status_code = get_resp.status_code

            if status_code < 400:
                return True

            self.logger.warning(f"é“¾æ¥è¿”å›çŠ¶æ€ç  {status_code}ï¼Œè·³è¿‡: {url}")
            return False
        except Exception as e:
            self.logger.warning(f"é“¾æ¥éªŒè¯å¤±è´¥ï¼Œè·³è¿‡: {url}ï¼ŒåŸå› : {e}")
            return False

    def build_usage_instructions(self, url):
        """ç”Ÿæˆä¸­æ–‡ä½¿ç”¨æ–¹æ³•è¯´æ˜"""
        domain = self.get_domain(url)
        if domain:
            return f"ä½¿ç”¨æ–¹æ³•ï¼šç‚¹å‡»ä¸‹æ–¹â€œå‰å¾€ä¼˜æƒ â€æŒ‰é’®è·³è½¬åˆ° {domain} å®˜æ–¹é¡µé¢ï¼ŒæŒ‰ç…§é¡µé¢æç¤ºå®Œæˆæ³¨å†Œæˆ–ä¸‹å•ï¼Œåœ¨éœ€è¦å¡«å†™æ¨èä¿¡æ¯æ—¶ä¿æŒé¡µé¢æ‰“å¼€å³å¯é¢†å–å¥–åŠ±ã€‚"
        return "ä½¿ç”¨æ–¹æ³•ï¼šç‚¹å‡»ä¸‹æ–¹â€œå‰å¾€ä¼˜æƒ â€æŒ‰é’®ï¼ŒæŒ‰ç…§é¡µé¢æç¤ºå®Œæˆæ³¨å†Œæˆ–ä¸‹å•å³å¯é¢†å–å¥–åŠ±ã€‚"

    def _sanitize_text(self, text):
        if not text:
            return ''
        return re.sub(r'\s+', ' ', text).strip()

    def extract_real_deal_url(self, detail_url):
        """ä»è¯¦æƒ…é¡µæå–çœŸå®çš„ä¼˜æƒ é“¾æ¥ï¼ˆéä¸­è½¬é¡µï¼‰"""
        try:
            # å¦‚æœå·²ç»æ˜¯å¤–éƒ¨é“¾æ¥ï¼Œç›´æ¥è¿”å›
            if 'latestfreestuff.co.uk' not in detail_url:
                return self.ensure_absolute_url(detail_url)

            # æ„å»ºå®Œæ•´URL
            full_url = self.ensure_absolute_url(detail_url)
                
            self.logger.info(f"æ­£åœ¨è·å–è¯¦æƒ…é¡µä»¥æå–çœŸå®é“¾æ¥: {full_url}")
            
            # è·å–è¯¦æƒ…é¡µå†…å®¹
            detail_content = self.get_page_content(full_url)
            if not detail_content:
                return detail_url
                
            # é¦–å…ˆæŸ¥æ‰¾ GET FREEBIE æŒ‰é’®é“¾æ¥
            get_freebie_pattern = r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>[^<]*GET\s+FREEBIE[^<]*</a>'
            freebie_matches = re.findall(get_freebie_pattern, detail_content, re.IGNORECASE)
            
            if freebie_matches:
                claim_url = freebie_matches[0]
                self.logger.info(f"æ‰¾åˆ° GET FREEBIE æŒ‰é’®é“¾æ¥: {claim_url}")
                
                # å¦‚æœæ˜¯ç”³è¯·é¡µé¢ï¼Œéœ€è¦è¿›ä¸€æ­¥æå–çœŸå®é“¾æ¥
                if 'latestfreestuff.co.uk/claim/' in claim_url:
                    real_url = self._extract_from_claim_page(claim_url)
                    if real_url and real_url != claim_url:
                        return self.ensure_absolute_url(real_url)
                elif 'latestfreestuff.co.uk' not in claim_url:
                    # å¦‚æœGET FREEBIEç›´æ¥æŒ‡å‘å¤–éƒ¨é“¾æ¥ï¼Œç›´æ¥è¿”å›
                    return self.ensure_absolute_url(claim_url)
                
            # é¦–å…ˆæŸ¥æ‰¾claimé¡µé¢é“¾æ¥ - è¿™é€šå¸¸åŒ…å«çœŸå®çš„ä¼˜æƒ é“¾æ¥
            claim_links = re.findall(r'href=["\']([^"\']*\/claim\/[^"\']*)["\']', detail_content, re.IGNORECASE)
            if claim_links:
                for claim_link in claim_links:
                    if claim_link.startswith('/'):
                        claim_url = self.base_url + claim_link
                    else:
                        claim_url = claim_link
                    
                    self.logger.info(f"æ‰¾åˆ°ç”³è¯·é¡µé¢ï¼Œæ­£åœ¨æå–çœŸå®é“¾æ¥: {claim_url}")
                    claim_url = self.ensure_absolute_url(claim_url)
                    real_link = self._extract_from_claim_page(claim_url)
                    if real_link:
                        return self.ensure_absolute_url(real_link)
                
            # é¦–å…ˆæŸ¥æ‰¾æœ€å¸¸è§çš„ä¼˜æƒ æŒ‰é’®é“¾æ¥ - æŒ‰ä¼˜å…ˆçº§æ’åº
            primary_patterns = [
                # ä¸»è¦çš„ä¼˜æƒ æŒ‰é’® - é€šå¸¸åŒ…å«ç‰¹å®šclass
                r'<a[^>]+class=["\'][^"\']*(?:deal-btn|offer-btn|get-deal|visit-store|claim-deal|btn-primary)[^"\']*["\'][^>]+href=["\']([^"\']+)["\']',
                # å¸¦æœ‰target="_blank"çš„å¤–éƒ¨é“¾æ¥ - æœ€å¯èƒ½æ˜¯çœŸå®é“¾æ¥
                r'<a[^>]+target=["\']_blank["\'][^>]+href=["\']([^"\']+)["\']',
                r'<a[^>]+href=["\']([^"\']+)["\'][^>]+target=["\']_blank["\']',
                # åŒ…å«"Get Deal"ã€"Visit Store"ç­‰æ–‡æœ¬çš„é“¾æ¥
                r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>[^<]*(?:Get Deal|Visit Store|Claim Deal|Shop Now|Get Offer|Grab Deal)[^<]*</a>',
                # åŒ…å«rel="nofollow"çš„å¤–éƒ¨é“¾æ¥
                r'<a[^>]+rel=["\']nofollow["\'][^>]+href=["\']([^"\']+)["\']',
                r'<a[^>]+href=["\']([^"\']+)["\'][^>]+rel=["\']nofollow["\']',
            ]
            
            # æ¬¡è¦æ¨¡å¼ - æ›´å¹¿æ³›çš„æœç´¢
            secondary_patterns = [
                # ä»»ä½•å¸¦æœ‰å¸¸è§æŒ‰é’®classçš„é“¾æ¥
                r'<a[^>]+class=["\'][^"\']*(?:btn|button)[^"\']*["\'][^>]+href=["\']([^"\']+)["\']',
                # åŒ…å«deal, offer, go, visitç­‰å…³é”®è¯çš„é“¾æ¥
                r'href=["\']([^"\']*(?:deal|offer|promo|discount)[^"\']*)["\']',
                r'href=["\']([^"\']*(?:go|visit|redirect)[^"\']*)["\']',
                # ä»»ä½•å¤–éƒ¨åŸŸåé“¾æ¥ï¼ˆéæœ¬ç«™é“¾æ¥ï¼‰
                r'href=["\'](https?://(?!(?:www\.)?latestfreestuff\.co\.uk)[a-zA-Z0-9][^"\']*?)["\']'
            ]
            
            # å…ˆå°è¯•ä¸»è¦æ¨¡å¼
            for i, pattern in enumerate(primary_patterns):
                matches = re.findall(pattern, detail_content, re.IGNORECASE)
                if matches:
                    for match in matches:
                        url = match if isinstance(match, str) else match[0]
                        if self.is_valid_deal_url(url):
                            self.logger.info(f"æ‰¾åˆ°ä¸»è¦ä¼˜æƒ é“¾æ¥ (æ¨¡å¼{i+1}): {url}")
                            return url
            
            # å†å°è¯•æ¬¡è¦æ¨¡å¼
            for i, pattern in enumerate(secondary_patterns):
                matches = re.findall(pattern, detail_content, re.IGNORECASE)
                if matches:
                    for match in matches:
                        # å¤„ç†æ­£åˆ™è¡¨è¾¾å¼å¯èƒ½è¿”å›çš„ä¸åŒæ ¼å¼
                        if isinstance(match, tuple):
                            url = match[0] if match[0] else (match[1] if len(match) > 1 else None)
                        else:
                            url = match
                            
                        if url and self.is_valid_deal_url(url):
                            self.logger.info(f"æ‰¾åˆ°æ¬¡è¦ä¼˜æƒ é“¾æ¥ (æ¨¡å¼{i+3}): {url}")
                            return url
                            
            # å°è¯•æŸ¥æ‰¾JavaScripté‡å®šå‘
            js_patterns = [
                r'window\.location\.href\s*=\s*["\']([^"\']+)["\']',
                r'window\.location\s*=\s*["\']([^"\']+)["\']',
                r'location\.href\s*=\s*["\']([^"\']+)["\']',
                r'document\.location\s*=\s*["\']([^"\']+)["\']',
            ]
            
            for pattern in js_patterns:
                js_matches = re.findall(pattern, detail_content, re.IGNORECASE)
                if js_matches:
                    url = js_matches[0]
                    if self.is_valid_deal_url(url):
                        self.logger.info(f"æ‰¾åˆ°JSé‡å®šå‘é“¾æ¥: {url}")
                        return url
            
            # æœ€åå°è¯•æŸ¥æ‰¾meta refreshé‡å®šå‘
            meta_pattern = r'<meta[^>]+http-equiv=["\']refresh["\'][^>]+content=["\'][^"\']*url=([^"\']+)["\']'
            meta_matches = re.findall(meta_pattern, detail_content, re.IGNORECASE)
            if meta_matches:
                url = meta_matches[0]
                if self.is_valid_deal_url(url):
                    self.logger.info(f"æ‰¾åˆ°metaé‡å®šå‘é“¾æ¥: {url}")
                    return url
            
            # å°è¯•æŸ¥æ‰¾iframe srcï¼ˆæœ‰äº›ç½‘ç«™ç”¨iframeåµŒå…¥å¤–éƒ¨é“¾æ¥ï¼‰
            iframe_pattern = r'<iframe[^>]+src=["\']([^"\']+)["\']'
            iframe_matches = re.findall(iframe_pattern, detail_content, re.IGNORECASE)
            if iframe_matches:
                for url in iframe_matches:
                    if self.is_valid_deal_url(url):
                        self.logger.info(f"æ‰¾åˆ°iframeé“¾æ¥: {url}")
                        return url
                        
            self.logger.warning(f"æœªæ‰¾åˆ°çœŸå®å¤–éƒ¨é“¾æ¥ï¼Œä½¿ç”¨è¯¦æƒ…é¡µé“¾æ¥: {full_url}")
            return self.ensure_absolute_url(full_url)

        except Exception as e:
            self.logger.error(f"æå–çœŸå®é“¾æ¥å¤±è´¥: {e}")
            return self.ensure_absolute_url(detail_url)

    def _extract_from_claim_page(self, claim_url):
        """ä»ç”³è¯·é¡µé¢æå–çœŸå®çš„ä¼˜æƒ é“¾æ¥"""
        try:
            self.logger.info(f"æ­£åœ¨ä»ç”³è¯·é¡µé¢æå–çœŸå®é“¾æ¥: {claim_url}")
            
            # è·å–ç”³è¯·é¡µé¢å†…å®¹
            claim_content = self.get_page_content(claim_url)
            if not claim_content:
                return self.ensure_absolute_url(claim_url)
                
            # æŸ¥æ‰¾ç”³è¯·é¡µé¢ä¸­çš„å¤–éƒ¨é“¾æ¥
            # ä¼˜å…ˆæŸ¥æ‰¾æ˜æ˜¾çš„å•†å®¶ç½‘ç«™é“¾æ¥
            external_patterns = [
                # æŸ¥æ‰¾ä¸»è¦çš„å¤–éƒ¨åŸŸåé“¾æ¥ï¼ˆæ’é™¤å¸¸è§çš„éä¼˜æƒ é“¾æ¥ï¼‰
                r'href=["\']((https?://(?!(?:www\.)?(?:latestfreestuff\.co\.uk|google\.com|facebook\.com|twitter\.com|instagram\.com|youtube\.com|analytics\.google\.com|fonts\.googleapis\.com))[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}[^"\']*?))["\']',
                # æŸ¥æ‰¾åŒ…å«å•†å®¶åç§°çš„é“¾æ¥æ¨¡å¼
                r'href=["\']((https?://[^"\']*(?:shop|store|buy|deal|offer|promo)[^"\']*?))["\']',
            ]
            
            for pattern in external_patterns:
                matches = re.findall(pattern, claim_content, re.IGNORECASE)
                for match in matches:
                    url = match if isinstance(match, str) else match[0]
                    # è¿›ä¸€æ­¥è¿‡æ»¤æ— æ•ˆé“¾æ¥
                    if self._is_valid_merchant_link(url):
                        self.logger.info(f"ä»ç”³è¯·é¡µé¢æ‰¾åˆ°çœŸå®ä¼˜æƒ é“¾æ¥: {url}")
                        return url
                        
            # å¦‚æœæ²¡æ‰¾åˆ°å¤–éƒ¨é“¾æ¥ï¼Œè¿”å›ç”³è¯·é¡µé¢æœ¬èº«
            return self.ensure_absolute_url(claim_url)
            
        except Exception as e:
            self.logger.error(f"ä»ç”³è¯·é¡µé¢æå–é“¾æ¥æ—¶å‡ºé”™: {e}")
            return self.ensure_absolute_url(claim_url)
    
    def _is_valid_merchant_link(self, url):
        """éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å•†å®¶é“¾æ¥"""
        if not url or len(url) < 10:
            return False
            
        url_lower = url.lower()
        
        # æ’é™¤æ˜æ˜¾çš„æ— æ•ˆé“¾æ¥
        invalid_patterns = [
            'javascript:', 'mailto:', 'tel:', '#', 'data:',
            'google.com', 'facebook.com', 'twitter.com', 'instagram.com',
            'youtube.com', 'linkedin.com', 'pinterest.com', 'tiktok.com',
            'fonts.googleapis.com', 'analytics.google.com', 'google-analytics.com',
            'googletagmanager.com', 'recaptcha', 'privacy-policy', 'terms-and-conditions',
            '.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico'
        ]
        
        for pattern in invalid_patterns:
            if pattern in url_lower:
                return False
                
        # å¿…é¡»æ˜¯å¤–éƒ¨é“¾æ¥
        if 'latestfreestuff.co.uk' in url_lower:
            return False
            
        # å¿…é¡»æ˜¯æœ‰æ•ˆçš„HTTP(S) URL
        if not url_lower.startswith(('http://', 'https://')):
            return False
            
        return True

    def is_valid_deal_url(self, url):
        """éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ä¼˜æƒ é“¾æ¥URL"""
        if not url or len(url) < 10:
            return False
        
        url_lower = url.lower()
        
        # æ’é™¤æ— æ•ˆé“¾æ¥
        invalid_patterns = [
            'javascript:', 'mailto:', 'tel:', '#', 'data:',
            'void(0)', 'about:blank',
            '.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.pdf'
        ]
        
        for pattern in invalid_patterns:
            if pattern in url_lower:
                return False
        
        # æ’é™¤æ˜æ˜¾çš„ç¤¾äº¤åª’ä½“åˆ†äº«é“¾æ¥
        social_share_patterns = [
            'share', 'sharer', 'intent/tweet', 'pin/create',
            'linkedin.com/in/', 'facebook.com/profile'
        ]
        
        for pattern in social_share_patterns:
            if pattern in url_lower:
                return False
                
        # æ’é™¤æ–‡ä»¶ä¸‹è½½é“¾æ¥ï¼ˆé€šå¸¸ä¸æ˜¯ä¼˜æƒ é“¾æ¥ï¼‰
        file_extensions = ['.pdf', '.doc', '.docx', '.zip', '.rar', '.exe', '.dmg']
        for ext in file_extensions:
            if url_lower.endswith(ext):
                return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å•†åº—/ä¼˜æƒ ç½‘ç«™åŸŸå
        # å¸¸è§çš„è‹±å›½è´­ç‰©ç½‘ç«™å’Œä¼˜æƒ ç½‘ç«™åŸŸå
        valid_domains = [
            'amazon.co.uk', 'amazon.com', 'ebay.co.uk', 'ebay.com',
            'argos.co.uk', 'currys.co.uk', 'johnlewis.com', 'marksandspencer.com',
            'tesco.com', 'asda.com', 'sainsburys.co.uk', 'morrisons.com',
            'boots.com', 'superdrug.com', 'next.co.uk', 'hm.com',
            'zara.com', 'asos.com', 'boohoo.com', 'prettylittlething.com',
            'topshop.com', 'newlook.com', 'primark.com', 'tkmaxx.com',
            'virginmedia.com', 'octopus.energy', 'bulb.co.uk', 'edf.co.uk',
            'groupon.co.uk', 'wowcher.co.uk', 'vouchercodes.co.uk',
            'hotukdeals.com', 'myvouchercodes.co.uk', 'retailmenot.com',
            'expedia.co.uk', 'booking.com', 'hotels.com', 'lastminute.com',
            'ryanair.com', 'easyjet.com', 'ba.com', 'trainline.com',
            'whitworths.co.uk', 'discoverysample.com'
        ]
        
        # è§£æåŸŸå
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # ç§»é™¤ www. å‰ç¼€
            if domain.startswith('www.'):
                domain = domain[4:]
                
            # å¦‚æœæ˜¯å·²çŸ¥çš„è´­ç‰©ç½‘ç«™ï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆçš„
            for valid_domain in valid_domains:
                if domain == valid_domain or domain.endswith('.' + valid_domain):
                    return True
                    
            # å¦‚æœåŒ…å«å¸¸è§çš„ä¼˜æƒ ç›¸å…³å…³é”®è¯ï¼Œä¹Ÿå¯èƒ½æ˜¯æœ‰æ•ˆçš„
            deal_keywords = [
                'deal', 'offer', 'discount', 'coupon', 'promo', 'sale',
                'shop', 'store', 'buy', 'checkout', 'cart', 'order',
                'voucher', 'code', 'cashback', 'reward'
            ]
            
            url_for_keywords = url_lower
            for keyword in deal_keywords:
                if keyword in url_for_keywords:
                    return True
                    
        except Exception:
            pass
            
        # å¦‚æœURLé•¿åº¦åˆç†ä¸”æ˜¯å¤–éƒ¨é“¾æ¥ï¼Œè®¤ä¸ºå¯èƒ½æ˜¯æœ‰æ•ˆçš„
        return len(url) > 15 and 'latestfreestuff.co.uk' not in url_lower

    def parse_deals(self, html_content):
        """è§£æä¼˜æƒ ä¿¡æ¯"""
        if not html_content:
            return []
            
        parser = DealParser()
        parser.feed(html_content)
        
        # æ¸…ç†å’ŒéªŒè¯æ•°æ®ï¼Œå¹¶è·å–çœŸå®é“¾æ¥
        valid_deals = []
        seen_urls = set()

        for deal in parser.deals:
            if len(valid_deals) >= self.max_deals:
                break

            if not self.is_valid_deal(deal):
                continue

            self.logger.info(f"å¤„ç†ä¼˜æƒ ã€Š{deal.get('title', 'æœªçŸ¥æ ‡é¢˜')}ã€‹...")

            # è·å–çœŸå®ä¼˜æƒ é“¾æ¥
            if 'detail_url' in deal:
                real_url = self.extract_real_deal_url(deal['detail_url'])
                deal['url'] = real_url
                deal['source_url'] = deal['detail_url']  # ä¿å­˜åŸå§‹è¯¦æƒ…é¡µé“¾æ¥

            deal = self.clean_deal_data(deal)
            url = deal.get('url')

            if not url or url in seen_urls:
                self.logger.warning(f"ä¼˜æƒ é“¾æ¥æ— æ•ˆæˆ–é‡å¤ï¼Œè·³è¿‡: {url}")
                continue

            if not self.verify_deal_link(url):
                self.logger.warning(f"æºç½‘ç«™æ— æ³•è®¿é—®ï¼Œç§»é™¤è¯¥ä¼˜æƒ : {url}")
                continue

            seen_urls.add(url)
            valid_deals.append(deal)

            if self.request_delay:
                time.sleep(self.request_delay)

        return valid_deals

    def is_valid_deal(self, deal):
        """éªŒè¯ä¼˜æƒ ä¿¡æ¯"""
        if not deal.get('title'):
            return False
        if len(deal['title']) < 5:
            return False
        if not deal.get('detail_url'):
            return False
        return True

    def clean_deal_data(self, deal):
        """æ¸…ç†ä¼˜æƒ æ•°æ®"""
        # æ¸…ç†æ ‡é¢˜
        if 'title' in deal:
            deal['title'] = re.sub(r'\s+', ' ', deal['title']).strip()
            
        # æ¸…ç†æè¿°
        if 'description' in deal:
            deal['description'] = re.sub(r'\s+', ' ', deal['description']).strip()
            deal['description'] = deal['description'][:300]  # é™åˆ¶é•¿åº¦

        # è§„èŒƒé“¾æ¥
        if 'detail_url' in deal:
            deal['detail_url'] = self.ensure_absolute_url(deal['detail_url'])

        if 'url' in deal:
            deal['url'] = self.ensure_absolute_url(deal['url'])

        # ä¿®å¤å›¾ç‰‡URL
        if 'image' in deal and not deal['image'].startswith('http'):
            if deal['image'].startswith('/'):
                deal['image'] = self.base_url + deal['image']
            else:
                deal['image'] = self.base_url + '/' + deal['image']

        # æ·»åŠ æ—¥æœŸ
        deal['date'] = datetime.now().strftime('%Y-%m-%d')

        # æ·»åŠ å•†å®¶åŸŸå
        deal['merchant'] = self.get_domain(deal.get('url')) or self.get_domain(deal.get('detail_url')) or 'æœªçŸ¥å•†å®¶'

        return deal

    def translate_deals(self, deals):
        """ç¿»è¯‘ä¼˜æƒ ä¿¡æ¯"""
        translated_deals = []

        for i, deal in enumerate(deals):
            self.logger.info(f"ç¿»è¯‘ç¬¬ {i+1}/{len(deals)} ä¸ªä¼˜æƒ ...")

            translated_deal = deal.copy()
            original_title = self._sanitize_text(deal.get('title', ''))
            original_desc = self._sanitize_text(deal.get('description', ''))

            if self.enable_translation:
                if original_title:
                    translated_deal['title_zh'] = self.translator.translate_to_chinese(original_title)
                if original_desc:
                    summary_text = self.translator.translate_to_chinese(original_desc)
                else:
                    summary_text = ''
            else:
                translated_deal['title_zh'] = original_title or deal.get('title', '')
                summary_text = original_desc

            if not translated_deal.get('title_zh'):
                translated_deal['title_zh'] = original_title or deal.get('title', '')

            summary_text = self._sanitize_text(summary_text)
            if not summary_text:
                summary_text = f"{translated_deal.get('title_zh') or original_title} é™æ—¶ä¼˜æƒ ï¼Œæ•°é‡æœ‰é™ï¼Œè®°å¾—å°½å¿«é¢†å–ã€‚"

            if len(summary_text) > 120:
                summary_text = summary_text[:120] + 'â€¦'

            translated_deal['summary_zh'] = summary_text
            translated_deal['description_zh'] = f"ä¼˜æƒ äº®ç‚¹ï¼š{summary_text}"
            translated_deal['usage'] = self.build_usage_instructions(translated_deal.get('url'))

            translated_deals.append(translated_deal)
            time.sleep(min(0.3, self.request_delay))  # é¿å…è¿‡äºé¢‘ç¹

        return translated_deals

    def save_deals(self, deals):
        """ä¿å­˜ä¼˜æƒ ä¿¡æ¯"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜JSON
        json_file = os.path.join(self.data_dir, f"enhanced_deals_{timestamp}.json")

        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(deals, f, ensure_ascii=False, indent=2)

        self.logger.info(f"å·²ä¿å­˜ {len(deals)} ä¸ªä¼˜æƒ åˆ° {json_file}")

        # ç”ŸæˆHTML
        html_content = self.generate_html(deals)
        html_file = os.path.join(self.data_dir, f"enhanced_deals_{timestamp}.html")

        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        return json_file, html_file

    def generate_html(self, deals):
        """ç”ŸæˆHTMLå†…å®¹"""
        update_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        html = [
            "    <section id=\"deals\" class=\"daily-deals\">",
            "        <div class=\"container\">",
            "            <div class=\"daily-deals-header\">",
            f"                <h2>ğŸ ä»Šæ—¥è‹±å›½ä¼˜æƒ ç²¾é€‰</h2>",
            f"                <p class=\"update-time\">ğŸ•’ æœ€æ–°æ›´æ–°ï¼š{update_time} ï½œ å·²ç­›é€‰ {len(deals)} æ¡çœŸå®ä¼˜æƒ </p>",
            "            </div>",
            "            <div class=\"deals-container\">"
        ]

        for deal in deals:
            title_zh = deal.get('title_zh') or deal.get('title') or 'ä»Šæ—¥ä¼˜æƒ '
            summary = deal.get('summary_zh') or deal.get('description_zh') or ''
            usage = deal.get('usage') or self.build_usage_instructions(deal.get('url'))
            url = deal.get('url', '#')
            merchant = deal.get('merchant', 'æœªçŸ¥å•†å®¶')
            date = deal.get('date', '')
            image = deal.get('image')

            card_html = ["                <article class=\"deal-card\">"]

            if image:
                card_html.append(f"                    <img src=\"{image}\" alt=\"{title_zh}\" loading=\"lazy\">")

            card_html.extend([
                f"                    <h3>{title_zh}</h3>",
                f"                    <p class=\"deal-summary\">{summary}</p>",
                f"                    <div class=\"deal-usage\">{usage}</div>",
                "                    <div class=\"deal-meta\">",
                f"                        <span>ğŸ“… {date}</span>",
                f"                        <span>ğŸŒ {merchant}</span>",
                "                    </div>",
                f"                    <a href=\"{url}\" target=\"_blank\" rel=\"noopener\" class=\"deal-link\">å‰å¾€ä¼˜æƒ </a>",
                "                </article>"
            ])

            html.extend(card_html)

        html.extend([
            "            </div>",
            "        </div>",
            "    </section>"
        ])

        return "\n".join(html)

    def run_crawler(self):
        """è¿è¡Œå¢å¼ºç‰ˆçˆ¬è™«"""
        self.logger.info("å¼€å§‹è¿è¡Œå¢å¼ºç‰ˆçˆ¬è™«ï¼Œè·å–çœŸå®ä¼˜æƒ é“¾æ¥...")
        
        # è·å–é¡µé¢
        html_content = self.get_page_content(self.base_url)
        if not html_content:
            self.logger.error("æ— æ³•è·å–ç½‘ç«™å†…å®¹")
            return []
            
        # è§£æä¼˜æƒ 
        deals = self.parse_deals(html_content)
        self.logger.info(f"æ‰¾åˆ° {len(deals)} ä¸ªä¼˜æƒ ")
        
        if not deals:
            return []
            
        # ç¿»è¯‘
        translated_deals = self.translate_deals(deals)
        
        # ä¿å­˜
        json_file, html_file = self.save_deals(translated_deals)
        
        self.logger.info(f"å¢å¼ºç‰ˆçˆ¬è™«å®Œæˆï¼æ–‡ä»¶: {json_file}, {html_file}")
        return translated_deals

def main():
    """ä¸»å‡½æ•°"""
    from urllib.parse import urlparse
    
    crawler = EnhancedFreeStuffCrawler()
    deals = crawler.run_crawler()
    
    if deals:
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(deals)} ä¸ªä¼˜æƒ ä¿¡æ¯ï¼ˆå«çœŸå®é“¾æ¥ï¼‰:")
        for i, deal in enumerate(deals, 1):
            title = deal.get('title_zh', deal.get('title', ''))
            url = deal.get('url', '')
            domain = urlparse(url).netloc if url.startswith('http') else 'æœ¬åœ°é“¾æ¥'
            print(f"{i}. {title}")
            print(f"   ğŸ”— {domain}")
    else:
        print("âŒ æœªè·å–åˆ°ä¼˜æƒ ä¿¡æ¯")

if __name__ == "__main__":
    main()
