import requests
import json
import re
import time
from datetime import datetime
from html.parser import HTMLParser
import logging
import os
from urllib.parse import urljoin, urlparse

# å¯¼å…¥é…ç½®
try:
    from enhanced_config import *
except ImportError:
    # å¦‚æœæ²¡æœ‰enhanced_configï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    BASE_URL = "https://www.latestfreestuff.co.uk"
    MAX_DEALS = 10
    REQUEST_DELAY = 2
    ENABLE_TRANSLATION = True

class SimpleTranslator:
    """ç®€å•çš„ç¿»è¯‘æœåŠ¡ï¼ˆå¯æ›¿æ¢ä¸ºå…¶ä»–ç¿»è¯‘APIï¼‰"""
    
    def __init__(self):
        self.cache = {}  # ç¿»è¯‘ç¼“å­˜
        
    def translate_to_chinese(self, text):
        """ç®€å•çš„è‹±è¯‘ä¸­ï¼ˆè¿™é‡Œä½¿ç”¨åŸºç¡€è¯æ±‡æ›¿æ¢ï¼Œå®é™…åº”ç”¨ä¸­å»ºè®®ä½¿ç”¨ä¸“ä¸šç¿»è¯‘APIï¼‰"""
        if not text or text in self.cache:
            return self.cache.get(text, text)
            
        # åŸºç¡€è¯æ±‡æ›¿æ¢ï¼ˆå¯æ‰©å±•ï¼‰
        translations = {
            'free': 'å…è´¹',
            'deal': 'ä¼˜æƒ ',
            'offer': 'ä¼˜æƒ ',
            'discount': 'æŠ˜æ‰£',
            'save': 'çœé’±',
            'sale': 'ä¿ƒé”€',
            'voucher': 'ä¼˜æƒ åˆ¸',
            'code': 'ä»£ç ',
            'cashback': 'è¿”ç°',
            'student': 'å­¦ç”Ÿ',
            'new': 'æ–°',
            'exclusive': 'ç‹¬å®¶',
            'limited': 'é™æ—¶',
            'today': 'ä»Šå¤©',
            'now': 'ç°åœ¨',
            'get': 'è·å¾—',
            'buy': 'è´­ä¹°',
            'shop': 'è´­ç‰©',
            'online': 'åœ¨çº¿',
            'delivery': 'é…é€',
            'shipping': 'è¿è´¹',
            'click': 'ç‚¹å‡»',
            'here': 'è¿™é‡Œ',
            'link': 'é“¾æ¥',
            'visit': 'è®¿é—®',
            'website': 'ç½‘ç«™',
            'store': 'å•†åº—',
            'price': 'ä»·æ ¼',
            'cheap': 'ä¾¿å®œ',
            'bargain': 'ä¾¿å®œè´§',
            'member': 'ä¼šå‘˜',
            'signup': 'æ³¨å†Œ',
            'register': 'æ³¨å†Œ',
            'account': 'è´¦æˆ·'
        }
        
        translated = text.lower()
        for en, zh in translations.items():
            translated = translated.replace(en, zh)
            
        # ä¿æŒåŸæ–‡çš„å¤§å°å†™ç»“æ„
        result = self._preserve_case_structure(text, translated)
        self.cache[text] = result
        return result
        
    def _preserve_case_structure(self, original, translated):
        """ä¿æŒåŸæ–‡çš„å¤§å°å†™ç»“æ„"""
        if not original:
            return translated
        if original.isupper():
            return translated.upper()
        if original.istitle():
            return translated.title()
        return translated

class DealParser(HTMLParser):
    """HTMLè§£æå™¨"""
    
    def __init__(self):
        super().__init__()
        self.deals = []
        self.current_deal = {}
        self.in_deal_container = False
        self.in_title = False
        self.in_description = False
        self.current_tag = None
        
    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        attrs_dict = dict(attrs)
        
        # æ£€æµ‹å¯èƒ½çš„ä¼˜æƒ å®¹å™¨
        if tag in ['article', 'div'] and any('deal' in str(v).lower() or 'post' in str(v).lower() 
                                           for v in attrs_dict.values()):
            self.in_deal_container = True
            self.current_deal = {}
            
        # æ£€æµ‹æ ‡é¢˜
        if tag in ['h1', 'h2', 'h3', 'h4'] and self.in_deal_container:
            self.in_title = True
            
        # æ£€æµ‹æè¿°
        if tag == 'p' and self.in_deal_container:
            self.in_description = True
            
        # æ£€æµ‹é“¾æ¥ - è·å–è¯¦æƒ…é¡µé“¾æ¥
        if tag == 'a' and self.in_deal_container and 'href' in attrs_dict:
            if 'detail_url' not in self.current_deal:
                self.current_deal['detail_url'] = attrs_dict['href']
                
        # æ£€æµ‹å›¾ç‰‡
        if tag == 'img' and self.in_deal_container:
            if 'src' in attrs_dict:
                self.current_deal['image'] = attrs_dict['src']
            elif 'data-src' in attrs_dict:
                self.current_deal['image'] = attrs_dict['data-src']
                
    def handle_endtag(self, tag):
        if tag in ['article', 'div'] and self.in_deal_container:
            if self.current_deal and 'title' in self.current_deal:
                self.deals.append(self.current_deal.copy())
            self.in_deal_container = False
            self.current_deal = {}
            
        if tag in ['h1', 'h2', 'h3', 'h4']:
            self.in_title = False
            
        if tag == 'p':
            self.in_description = False
            
    def handle_data(self, data):
        data = data.strip()
        if not data:
            return
            
        if self.in_title and self.in_deal_container:
            self.current_deal['title'] = data
            
        if self.in_description and self.in_deal_container:
            if 'description' not in self.current_deal:
                self.current_deal['description'] = data
            else:
                self.current_deal['description'] += ' ' + data

class EnhancedFreeStuffCrawler:
    """å¢å¼ºç‰ˆä¼˜æƒ çˆ¬è™« - è·å–çœŸå®ä¼˜æƒ é“¾æ¥"""
    
    def __init__(self):
        self.base_url = "https://www.latestfreestuff.co.uk"
        self.translator = SimpleTranslator()
        self.session = requests.Session()
        self.setup_logging()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        self.session.headers.update(self.headers)
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('enhanced_crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def get_page_content(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            self.logger.info(f"æ­£åœ¨è·å–é¡µé¢: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            self.logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None

    def extract_real_deal_url(self, detail_url):
        """ä»è¯¦æƒ…é¡µæå–çœŸå®çš„ä¼˜æƒ é“¾æ¥ï¼ˆéä¸­è½¬é¡µï¼‰"""
        try:
            # å¦‚æœå·²ç»æ˜¯å¤–éƒ¨é“¾æ¥ï¼Œç›´æ¥è¿”å›
            if 'latestfreestuff.co.uk' not in detail_url:
                return detail_url
                
            # æ„å»ºå®Œæ•´URL
            if detail_url.startswith('/'):
                full_url = self.base_url + detail_url
            else:
                full_url = detail_url
                
            self.logger.info(f"æ­£åœ¨è·å–è¯¦æƒ…é¡µä»¥æå–çœŸå®é“¾æ¥: {full_url}")
            
            # è·å–è¯¦æƒ…é¡µå†…å®¹
            detail_content = self.get_page_content(full_url)
            if not detail_content:
                return detail_url
                
            # é¦–å…ˆæŸ¥æ‰¾ GET FREEBIE æŒ‰é’®é“¾æ¥
            get_freebie_pattern = r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>[^<]*GET\s+FREEBIE[^<]*</a>'
            freebie_matches = re.findall(get_freebie_pattern, detail_content, re.IGNORECASE)
            
            if freebie_matches:
                claim_url = freebie_matches[0]
                self.logger.info(f"æ‰¾åˆ° GET FREEBIE æŒ‰é’®é“¾æ¥: {claim_url}")
                
                # å¦‚æœæ˜¯ç”³è¯·é¡µé¢ï¼Œéœ€è¦è¿›ä¸€æ­¥æå–çœŸå®é“¾æ¥
                if 'latestfreestuff.co.uk/claim/' in claim_url:
                    real_url = self._extract_from_claim_page(claim_url)
                    if real_url and real_url != claim_url:
                        return real_url
                elif 'latestfreestuff.co.uk' not in claim_url:
                    # å¦‚æœGET FREEBIEç›´æ¥æŒ‡å‘å¤–éƒ¨é“¾æ¥ï¼Œç›´æ¥è¿”å›
                    return claim_url
                
            # é¦–å…ˆæŸ¥æ‰¾claimé¡µé¢é“¾æ¥ - è¿™é€šå¸¸åŒ…å«çœŸå®çš„ä¼˜æƒ é“¾æ¥
            claim_links = re.findall(r'href=["\']([^"\']*\/claim\/[^"\']*)["\']', detail_content, re.IGNORECASE)
            if claim_links:
                for claim_link in claim_links:
                    if claim_link.startswith('/'):
                        claim_url = self.base_url + claim_link
                    else:
                        claim_url = claim_link
                    
                    self.logger.info(f"æ‰¾åˆ°ç”³è¯·é¡µé¢ï¼Œæ­£åœ¨æå–çœŸå®é“¾æ¥: {claim_url}")
                    claim_content = self.get_page_content(claim_url)
                    
                    if claim_content:
                        # ä»ç”³è¯·é¡µé¢æå–å¤–éƒ¨é“¾æ¥
                        real_link = self._extract_from_claim_page(claim_content)
                        if real_link:
                            return real_link
                
            # é¦–å…ˆæŸ¥æ‰¾æœ€å¸¸è§çš„ä¼˜æƒ æŒ‰é’®é“¾æ¥ - æŒ‰ä¼˜å…ˆçº§æ’åº
            primary_patterns = [
                # ä¸»è¦çš„ä¼˜æƒ æŒ‰é’® - é€šå¸¸åŒ…å«ç‰¹å®šclass
                r'<a[^>]+class=["\'][^"\']*(?:deal-btn|offer-btn|get-deal|visit-store|claim-deal|btn-primary)[^"\']*["\'][^>]+href=["\']([^"\']+)["\']',
                # å¸¦æœ‰target="_blank"çš„å¤–éƒ¨é“¾æ¥ - æœ€å¯èƒ½æ˜¯çœŸå®é“¾æ¥
                r'<a[^>]+target=["\']_blank["\'][^>]+href=["\']([^"\']+)["\']',
                r'<a[^>]+href=["\']([^"\']+)["\'][^>]+target=["\']_blank["\']',
                # åŒ…å«"Get Deal"ã€"Visit Store"ç­‰æ–‡æœ¬çš„é“¾æ¥
                r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>[^<]*(?:Get Deal|Visit Store|Claim Deal|Shop Now|Get Offer|Grab Deal)[^<]*</a>',
                # åŒ…å«rel="nofollow"çš„å¤–éƒ¨é“¾æ¥
                r'<a[^>]+rel=["\']nofollow["\'][^>]+href=["\']([^"\']+)["\']',
                r'<a[^>]+href=["\']([^"\']+)["\'][^>]+rel=["\']nofollow["\']',
            ]
            
            # æ¬¡è¦æ¨¡å¼ - æ›´å¹¿æ³›çš„æœç´¢
            secondary_patterns = [
                # ä»»ä½•å¸¦æœ‰å¸¸è§æŒ‰é’®classçš„é“¾æ¥
                r'<a[^>]+class=["\'][^"\']*(?:btn|button)[^"\']*["\'][^>]+href=["\']([^"\']+)["\']',
                # åŒ…å«deal, offer, go, visitç­‰å…³é”®è¯çš„é“¾æ¥
                r'href=["\']([^"\']*(?:deal|offer|promo|discount)[^"\']*)["\']',
                r'href=["\']([^"\']*(?:go|visit|redirect)[^"\']*)["\']',
                # ä»»ä½•å¤–éƒ¨åŸŸåé“¾æ¥ï¼ˆéæœ¬ç«™é“¾æ¥ï¼‰
                r'href=["\'](https?://(?!(?:www\.)?latestfreestuff\.co\.uk)[a-zA-Z0-9][^"\']*?)["\']'
            ]
            
            # å…ˆå°è¯•ä¸»è¦æ¨¡å¼
            for i, pattern in enumerate(primary_patterns):
                matches = re.findall(pattern, detail_content, re.IGNORECASE)
                if matches:
                    for match in matches:
                        url = match if isinstance(match, str) else match[0]
                        if self.is_valid_deal_url(url):
                            self.logger.info(f"æ‰¾åˆ°ä¸»è¦ä¼˜æƒ é“¾æ¥ (æ¨¡å¼{i+1}): {url}")
                            return url
            
            # å†å°è¯•æ¬¡è¦æ¨¡å¼
            for i, pattern in enumerate(secondary_patterns):
                matches = re.findall(pattern, detail_content, re.IGNORECASE)
                if matches:
                    for match in matches:
                        # å¤„ç†æ­£åˆ™è¡¨è¾¾å¼å¯èƒ½è¿”å›çš„ä¸åŒæ ¼å¼
                        if isinstance(match, tuple):
                            url = match[0] if match[0] else (match[1] if len(match) > 1 else None)
                        else:
                            url = match
                            
                        if url and self.is_valid_deal_url(url):
                            self.logger.info(f"æ‰¾åˆ°æ¬¡è¦ä¼˜æƒ é“¾æ¥ (æ¨¡å¼{i+3}): {url}")
                            return url
                            
            # å°è¯•æŸ¥æ‰¾JavaScripté‡å®šå‘
            js_patterns = [
                r'window\.location\.href\s*=\s*["\']([^"\']+)["\']',
                r'window\.location\s*=\s*["\']([^"\']+)["\']',
                r'location\.href\s*=\s*["\']([^"\']+)["\']',
                r'document\.location\s*=\s*["\']([^"\']+)["\']',
            ]
            
            for pattern in js_patterns:
                js_matches = re.findall(pattern, detail_content, re.IGNORECASE)
                if js_matches:
                    url = js_matches[0]
                    if self.is_valid_deal_url(url):
                        self.logger.info(f"æ‰¾åˆ°JSé‡å®šå‘é“¾æ¥: {url}")
                        return url
            
            # æœ€åå°è¯•æŸ¥æ‰¾meta refreshé‡å®šå‘
            meta_pattern = r'<meta[^>]+http-equiv=["\']refresh["\'][^>]+content=["\'][^"\']*url=([^"\']+)["\']'
            meta_matches = re.findall(meta_pattern, detail_content, re.IGNORECASE)
            if meta_matches:
                url = meta_matches[0]
                if self.is_valid_deal_url(url):
                    self.logger.info(f"æ‰¾åˆ°metaé‡å®šå‘é“¾æ¥: {url}")
                    return url
            
            # å°è¯•æŸ¥æ‰¾iframe srcï¼ˆæœ‰äº›ç½‘ç«™ç”¨iframeåµŒå…¥å¤–éƒ¨é“¾æ¥ï¼‰
            iframe_pattern = r'<iframe[^>]+src=["\']([^"\']+)["\']'
            iframe_matches = re.findall(iframe_pattern, detail_content, re.IGNORECASE)
            if iframe_matches:
                for url in iframe_matches:
                    if self.is_valid_deal_url(url):
                        self.logger.info(f"æ‰¾åˆ°iframeé“¾æ¥: {url}")
                        return url
                        
            self.logger.warning(f"æœªæ‰¾åˆ°çœŸå®å¤–éƒ¨é“¾æ¥ï¼Œä½¿ç”¨è¯¦æƒ…é¡µé“¾æ¥: {full_url}")
            return full_url
            
        except Exception as e:
            self.logger.error(f"æå–çœŸå®é“¾æ¥å¤±è´¥: {e}")
            return detail_url

    def _extract_from_claim_page(self, claim_url):
        """ä»ç”³è¯·é¡µé¢æå–çœŸå®çš„ä¼˜æƒ é“¾æ¥"""
        try:
            self.logger.info(f"æ­£åœ¨ä»ç”³è¯·é¡µé¢æå–çœŸå®é“¾æ¥: {claim_url}")
            
            # è·å–ç”³è¯·é¡µé¢å†…å®¹
            claim_content = self.get_page_content(claim_url)
            if not claim_content:
                return claim_url
                
            # æŸ¥æ‰¾ç”³è¯·é¡µé¢ä¸­çš„å¤–éƒ¨é“¾æ¥
            # ä¼˜å…ˆæŸ¥æ‰¾æ˜æ˜¾çš„å•†å®¶ç½‘ç«™é“¾æ¥
            external_patterns = [
                # æŸ¥æ‰¾ä¸»è¦çš„å¤–éƒ¨åŸŸåé“¾æ¥ï¼ˆæ’é™¤å¸¸è§çš„éä¼˜æƒ é“¾æ¥ï¼‰
                r'href=["\']((https?://(?!(?:www\.)?(?:latestfreestuff\.co\.uk|google\.com|facebook\.com|twitter\.com|instagram\.com|youtube\.com|analytics\.google\.com|fonts\.googleapis\.com))[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}[^"\']*?))["\']',
                # æŸ¥æ‰¾åŒ…å«å•†å®¶åç§°çš„é“¾æ¥æ¨¡å¼
                r'href=["\']((https?://[^"\']*(?:shop|store|buy|deal|offer|promo)[^"\']*?))["\']',
            ]
            
            for pattern in external_patterns:
                matches = re.findall(pattern, claim_content, re.IGNORECASE)
                for match in matches:
                    url = match if isinstance(match, str) else match[0]
                    # è¿›ä¸€æ­¥è¿‡æ»¤æ— æ•ˆé“¾æ¥
                    if self._is_valid_merchant_link(url):
                        self.logger.info(f"ä»ç”³è¯·é¡µé¢æ‰¾åˆ°çœŸå®ä¼˜æƒ é“¾æ¥: {url}")
                        return url
                        
            # å¦‚æœæ²¡æ‰¾åˆ°å¤–éƒ¨é“¾æ¥ï¼Œè¿”å›ç”³è¯·é¡µé¢æœ¬èº«
            return claim_url
            
        except Exception as e:
            self.logger.error(f"ä»ç”³è¯·é¡µé¢æå–é“¾æ¥æ—¶å‡ºé”™: {e}")
            return claim_url
    
    def _is_valid_merchant_link(self, url):
        """éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å•†å®¶é“¾æ¥"""
        if not url or len(url) < 10:
            return False
            
        url_lower = url.lower()
        
        # æ’é™¤æ˜æ˜¾çš„æ— æ•ˆé“¾æ¥
        invalid_patterns = [
            'javascript:', 'mailto:', 'tel:', '#', 'data:',
            'google.com', 'facebook.com', 'twitter.com', 'instagram.com',
            'youtube.com', 'linkedin.com', 'pinterest.com', 'tiktok.com',
            'fonts.googleapis.com', 'analytics.google.com', 'google-analytics.com',
            'googletagmanager.com', 'recaptcha', 'privacy-policy', 'terms-and-conditions',
            '.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico'
        ]
        
        for pattern in invalid_patterns:
            if pattern in url_lower:
                return False
                
        # å¿…é¡»æ˜¯å¤–éƒ¨é“¾æ¥
        if 'latestfreestuff.co.uk' in url_lower:
            return False
            
        # å¿…é¡»æ˜¯æœ‰æ•ˆçš„HTTP(S) URL
        if not url_lower.startswith(('http://', 'https://')):
            return False
            
        return True

    def is_valid_deal_url(self, url):
        """éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ä¼˜æƒ é“¾æ¥URL"""
        if not url or len(url) < 10:
            return False
        
        url_lower = url.lower()
        
        # æ’é™¤æ— æ•ˆé“¾æ¥
        invalid_patterns = [
            'javascript:', 'mailto:', 'tel:', '#', 'data:',
            'void(0)', 'about:blank',
            '.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.pdf'
        ]
        
        for pattern in invalid_patterns:
            if pattern in url_lower:
                return False
        
        # æ’é™¤æ˜æ˜¾çš„ç¤¾äº¤åª’ä½“åˆ†äº«é“¾æ¥
        social_share_patterns = [
            'share', 'sharer', 'intent/tweet', 'pin/create',
            'linkedin.com/in/', 'facebook.com/profile'
        ]
        
        for pattern in social_share_patterns:
            if pattern in url_lower:
                return False
                
        # æ’é™¤æ–‡ä»¶ä¸‹è½½é“¾æ¥ï¼ˆé€šå¸¸ä¸æ˜¯ä¼˜æƒ é“¾æ¥ï¼‰
        file_extensions = ['.pdf', '.doc', '.docx', '.zip', '.rar', '.exe', '.dmg']
        for ext in file_extensions:
            if url_lower.endswith(ext):
                return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å•†åº—/ä¼˜æƒ ç½‘ç«™åŸŸå
        # å¸¸è§çš„è‹±å›½è´­ç‰©ç½‘ç«™å’Œä¼˜æƒ ç½‘ç«™åŸŸå
        valid_domains = [
            'amazon.co.uk', 'amazon.com', 'ebay.co.uk', 'ebay.com',
            'argos.co.uk', 'currys.co.uk', 'johnlewis.com', 'marksandspencer.com',
            'tesco.com', 'asda.com', 'sainsburys.co.uk', 'morrisons.com',
            'boots.com', 'superdrug.com', 'next.co.uk', 'hm.com',
            'zara.com', 'asos.com', 'boohoo.com', 'prettylittlething.com',
            'topshop.com', 'newlook.com', 'primark.com', 'tkmaxx.com',
            'virginmedia.com', 'octopus.energy', 'bulb.co.uk', 'edf.co.uk',
            'groupon.co.uk', 'wowcher.co.uk', 'vouchercodes.co.uk',
            'hotukdeals.com', 'myvouchercodes.co.uk', 'retailmenot.com',
            'expedia.co.uk', 'booking.com', 'hotels.com', 'lastminute.com',
            'ryanair.com', 'easyjet.com', 'ba.com', 'trainline.com',
            'whitworths.co.uk', 'discoverysample.com'
        ]
        
        # è§£æåŸŸå
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # ç§»é™¤ www. å‰ç¼€
            if domain.startswith('www.'):
                domain = domain[4:]
                
            # å¦‚æœæ˜¯å·²çŸ¥çš„è´­ç‰©ç½‘ç«™ï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆçš„
            for valid_domain in valid_domains:
                if domain == valid_domain or domain.endswith('.' + valid_domain):
                    return True
                    
            # å¦‚æœåŒ…å«å¸¸è§çš„ä¼˜æƒ ç›¸å…³å…³é”®è¯ï¼Œä¹Ÿå¯èƒ½æ˜¯æœ‰æ•ˆçš„
            deal_keywords = [
                'deal', 'offer', 'discount', 'coupon', 'promo', 'sale',
                'shop', 'store', 'buy', 'checkout', 'cart', 'order',
                'voucher', 'code', 'cashback', 'reward'
            ]
            
            url_for_keywords = url_lower
            for keyword in deal_keywords:
                if keyword in url_for_keywords:
                    return True
                    
        except Exception:
            pass
            
        # å¦‚æœURLé•¿åº¦åˆç†ä¸”æ˜¯å¤–éƒ¨é“¾æ¥ï¼Œè®¤ä¸ºå¯èƒ½æ˜¯æœ‰æ•ˆçš„
        return len(url) > 15 and 'latestfreestuff.co.uk' not in url_lower

    def parse_deals(self, html_content):
        """è§£æä¼˜æƒ ä¿¡æ¯"""
        if not html_content:
            return []
            
        parser = DealParser()
        parser.feed(html_content)
        
        # æ¸…ç†å’ŒéªŒè¯æ•°æ®ï¼Œå¹¶è·å–çœŸå®é“¾æ¥
        valid_deals = []
        for i, deal in enumerate(parser.deals[:5]):  # é™åˆ¶æœ€å¤š5ä¸ªï¼Œé¿å…è¿‡å¤šè¯·æ±‚
            if self.is_valid_deal(deal):
                self.logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(parser.deals[:5])} ä¸ªä¼˜æƒ ...")
                
                # è·å–çœŸå®ä¼˜æƒ é“¾æ¥
                if 'detail_url' in deal:
                    real_url = self.extract_real_deal_url(deal['detail_url'])
                    deal['url'] = real_url
                    deal['source_url'] = deal['detail_url']  # ä¿å­˜åŸå§‹è¯¦æƒ…é¡µé“¾æ¥
                    
                deal = self.clean_deal_data(deal)
                valid_deals.append(deal)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç¹è¯·æ±‚
                time.sleep(2)
                
        return valid_deals

    def is_valid_deal(self, deal):
        """éªŒè¯ä¼˜æƒ ä¿¡æ¯"""
        if not deal.get('title'):
            return False
        if len(deal['title']) < 5:
            return False
        if not deal.get('detail_url'):
            return False
        return True

    def clean_deal_data(self, deal):
        """æ¸…ç†ä¼˜æƒ æ•°æ®"""
        # æ¸…ç†æ ‡é¢˜
        if 'title' in deal:
            deal['title'] = re.sub(r'\s+', ' ', deal['title']).strip()
            
        # æ¸…ç†æè¿°
        if 'description' in deal:
            deal['description'] = re.sub(r'\s+', ' ', deal['description']).strip()
            deal['description'] = deal['description'][:300]  # é™åˆ¶é•¿åº¦
            
        # ä¿®å¤å›¾ç‰‡URL
        if 'image' in deal and not deal['image'].startswith('http'):
            if deal['image'].startswith('/'):
                deal['image'] = self.base_url + deal['image']
            else:
                deal['image'] = self.base_url + '/' + deal['image']
                
        # æ·»åŠ æ—¥æœŸ
        deal['date'] = datetime.now().strftime('%Y-%m-%d')
        
        return deal

    def translate_deals(self, deals):
        """ç¿»è¯‘ä¼˜æƒ ä¿¡æ¯"""
        translated_deals = []
        
        for i, deal in enumerate(deals):
            self.logger.info(f"ç¿»è¯‘ç¬¬ {i+1}/{len(deals)} ä¸ªä¼˜æƒ ...")
            
            translated_deal = deal.copy()
            
            # ç¿»è¯‘æ ‡é¢˜
            if 'title' in deal:
                translated_deal['title_zh'] = self.translator.translate_to_chinese(deal['title'])
                
            # ç¿»è¯‘æè¿°
            if 'description' in deal:
                translated_deal['description_zh'] = self.translator.translate_to_chinese(deal['description'])
                
            translated_deals.append(translated_deal)
            time.sleep(0.5)  # é¿å…è¿‡äºé¢‘ç¹
            
        return translated_deals

    def save_deals(self, deals):
        """ä¿å­˜ä¼˜æƒ ä¿¡æ¯"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜JSON
        os.makedirs('data', exist_ok=True)
        json_file = f"data/enhanced_deals_{timestamp}.json"
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(deals, f, ensure_ascii=False, indent=2)
            
        self.logger.info(f"å·²ä¿å­˜ {len(deals)} ä¸ªä¼˜æƒ åˆ° {json_file}")
        
        # ç”ŸæˆHTML
        html_content = self.generate_html(deals)
        html_file = f"data/enhanced_deals_{timestamp}.html"
        
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
            
        return json_file, html_file

    def generate_html(self, deals):
        """ç”ŸæˆHTMLå†…å®¹"""
        html = f"""
        <section class="daily-deals">
            <div class="container">
                <div class="daily-deals-section">
                    <h2>ğŸ ä»Šæ—¥è‹±å›½ä¼˜æƒ ç²¾é€‰ï¼ˆçœŸå®é“¾æ¥ç‰ˆï¼‰</h2>
                    <p class="update-time">æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                    <div class="deals-container">
        """
        
        for deal in deals:
            title_zh = deal.get('title_zh', deal.get('title', ''))
            desc_zh = deal.get('description_zh', deal.get('description', ''))
            if len(desc_zh) > 100:
                desc_zh = desc_zh[:100] + "..."
            
            # æ˜¾ç¤ºçœŸå®é“¾æ¥åŸŸå
            url = deal.get('url', '#')
            domain = urlparse(url).netloc if url.startswith('http') else 'æœªçŸ¥'
            
            html += f"""
            <div class="deal-item">
                <h3>{title_zh}</h3>
                <p>{desc_zh}</p>
                <div class="deal-meta">
                    <span class="date">ğŸ“… {deal.get('date', '')}</span>
                    <span class="source">ğŸ”— {domain}</span>
                    <a href="{url}" target="_blank" class="deal-link">è®¿é—®ä¼˜æƒ </a>
                </div>
            </div>
            """
            
        html += """
                    </div>
                    <div class="deal-note">
                        <p>ğŸ’¡ æ‰€æœ‰é“¾æ¥å·²è§£æä¸ºçœŸå®ä¼˜æƒ åœ°å€ï¼Œç‚¹å‡»ç›´æ¥å‰å¾€å•†å®¶å®˜ç½‘</p>
                    </div>
                </div>
            </div>
        </section>
        """
        
        return html

    def run_crawler(self):
        """è¿è¡Œå¢å¼ºç‰ˆçˆ¬è™«"""
        self.logger.info("å¼€å§‹è¿è¡Œå¢å¼ºç‰ˆçˆ¬è™«ï¼Œè·å–çœŸå®ä¼˜æƒ é“¾æ¥...")
        
        # è·å–é¡µé¢
        html_content = self.get_page_content(self.base_url)
        if not html_content:
            self.logger.error("æ— æ³•è·å–ç½‘ç«™å†…å®¹")
            return []
            
        # è§£æä¼˜æƒ 
        deals = self.parse_deals(html_content)
        self.logger.info(f"æ‰¾åˆ° {len(deals)} ä¸ªä¼˜æƒ ")
        
        if not deals:
            return []
            
        # ç¿»è¯‘
        translated_deals = self.translate_deals(deals)
        
        # ä¿å­˜
        json_file, html_file = self.save_deals(translated_deals)
        
        self.logger.info(f"å¢å¼ºç‰ˆçˆ¬è™«å®Œæˆï¼æ–‡ä»¶: {json_file}, {html_file}")
        return translated_deals

def main():
    """ä¸»å‡½æ•°"""
    from urllib.parse import urlparse
    
    crawler = EnhancedFreeStuffCrawler()
    deals = crawler.run_crawler()
    
    if deals:
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(deals)} ä¸ªä¼˜æƒ ä¿¡æ¯ï¼ˆå«çœŸå®é“¾æ¥ï¼‰:")
        for i, deal in enumerate(deals, 1):
            title = deal.get('title_zh', deal.get('title', ''))
            url = deal.get('url', '')
            domain = urlparse(url).netloc if url.startswith('http') else 'æœ¬åœ°é“¾æ¥'
            print(f"{i}. {title}")
            print(f"   ğŸ”— {domain}")
    else:
        print("âŒ æœªè·å–åˆ°ä¼˜æƒ ä¿¡æ¯")

if __name__ == "__main__":
    main()
