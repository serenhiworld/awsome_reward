#!/usr/bin/env python3
"""
è‹±å›½ä¼˜æƒ çˆ¬è™«ç®¡ç†å·¥å…·
æ”¯æŒç«‹å³è¿è¡Œã€å®šæ—¶è°ƒåº¦ã€ç½‘ç«™æ›´æ–°ç­‰åŠŸèƒ½
"""

import os
import sys
import json
import subprocess
import schedule
import time
from datetime import datetime
import argparse

def run_simple_crawler():
    """è¿è¡Œç®€å•çˆ¬è™«"""
    try:
        result = subprocess.run([sys.executable, 'simple_crawler.py'], 
                              capture_output=True, text=True, cwd='crawler')
        if result.returncode == 0:
            print("âœ… çˆ¬è™«è¿è¡ŒæˆåŠŸ")
            print(result.stdout)
        else:
            print("âŒ çˆ¬è™«è¿è¡Œå¤±è´¥")
            print(result.stderr)
        return result.returncode == 0
    except Exception as e:
        print(f"âŒ è¿è¡Œçˆ¬è™«æ—¶å‡ºé”™: {e}")
        return False

def update_website():
    """æ›´æ–°ç½‘ç«™å†…å®¹"""
    try:
        # æŸ¥æ‰¾æœ€æ–°çš„HTMLæ–‡ä»¶
        data_dir = "crawler/data"
        if not os.path.exists(data_dir):
            print("âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨")
            return False
            
        html_files = [f for f in os.listdir(data_dir) if f.endswith('.html')]
        if not html_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°HTMLæ–‡ä»¶")
            return False
            
        # è·å–æœ€æ–°æ–‡ä»¶
        latest_html = max(html_files, key=lambda f: os.path.getctime(os.path.join(data_dir, f)))
        html_path = os.path.join(data_dir, latest_html)
        
        # è¯»å–å†…å®¹
        with open(html_path, 'r', encoding='utf-8') as f:
            deals_html = f.read()
            
        # è¯»å–ä¸»ç½‘ç«™æ–‡ä»¶
        main_html_path = "index.html"
        if not os.path.exists(main_html_path):
            print("âŒ ä¸»ç½‘ç«™HTMLæ–‡ä»¶ä¸å­˜åœ¨")
            return False
            
        with open(main_html_path, 'r', encoding='utf-8') as f:
            main_content = f.read()
            
        # å¤‡ä»½åŸæ–‡ä»¶
        backup_path = f"index_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(main_content)
            
        # æŸ¥æ‰¾æ’å…¥ä½ç½®å¹¶æ’å…¥å†…å®¹
        insert_marker = '<section id="benefits" class="benefits">'
        if insert_marker in main_content:
            # ç§»é™¤æ—§çš„çˆ¬è™«å†…å®¹
            import re
            main_content = re.sub(
                r'<div class="daily-deals-section">.*?</div>\\s*</div>',
                '',
                main_content,
                flags=re.DOTALL
            )
            
            # æ’å…¥æ–°å†…å®¹
            new_content = main_content.replace(
                insert_marker,
                deals_html + '\\n\\n    ' + insert_marker
            )
            
            # å†™å…¥æ›´æ–°åçš„å†…å®¹
            with open(main_html_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
                
            print(f"âœ… ç½‘ç«™æ›´æ–°æˆåŠŸï¼Œå¤‡ä»½æ–‡ä»¶: {backup_path}")
            return True
        else:
            print("âŒ æœªæ‰¾åˆ°æ’å…¥ä½ç½®")
            return False
            
    except Exception as e:
        print(f"âŒ æ›´æ–°ç½‘ç«™æ—¶å‡ºé”™: {e}")
        return False

def scheduled_task():
    """å®šæ—¶ä»»åŠ¡"""
    print(f"\\nğŸ•’ å®šæ—¶ä»»åŠ¡å¼€å§‹ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # è¿è¡Œçˆ¬è™«
    if run_simple_crawler():
        # æ›´æ–°ç½‘ç«™
        update_website()
        print("âœ… å®šæ—¶ä»»åŠ¡å®Œæˆ")
    else:
        print("âŒ å®šæ—¶ä»»åŠ¡å¤±è´¥")

def start_scheduler():
    """å¯åŠ¨å®šæ—¶è°ƒåº¦å™¨"""
    print("â° å¯åŠ¨å®šæ—¶è°ƒåº¦å™¨...")
    
    # è®¾ç½®å®šæ—¶ä»»åŠ¡
    schedule.every().day.at("09:00").do(scheduled_task)  # ä¸Šåˆ9ç‚¹
    schedule.every().day.at("18:00").do(scheduled_task)  # ä¸‹åˆ6ç‚¹
    schedule.every().day.at("23:00").do(scheduled_task)  # æ™šä¸Š11ç‚¹
    
    # ç«‹å³è¿è¡Œä¸€æ¬¡
    print("ğŸ”„ ç«‹å³è¿è¡Œä¸€æ¬¡...")
    scheduled_task()
    
    print("â° å®šæ—¶è°ƒåº¦å™¨å·²å¯åŠ¨ï¼Œç­‰å¾…å®šæ—¶ä»»åŠ¡...")
    print("ğŸ“… è°ƒåº¦æ—¶é—´: 09:00, 18:00, 23:00")
    print("æŒ‰ Ctrl+C åœæ­¢")
    
    try:
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    except KeyboardInterrupt:
        print("\\nâ¹ï¸  å®šæ—¶è°ƒåº¦å™¨å·²åœæ­¢")

def install_dependencies():
    """å®‰è£…ä¾èµ–"""
    print("ğŸ“¦ å®‰è£…Pythonä¾èµ–...")
    try:
        subprocess.run([sys.executable, '-m', 'pip', 'install', 'requests', 'schedule'], check=True)
        print("âœ… ä¾èµ–å®‰è£…å®Œæˆ")
    except subprocess.CalledProcessError:
        print("âŒ ä¾èµ–å®‰è£…å¤±è´¥")

def show_status():
    """æ˜¾ç¤ºçŠ¶æ€"""
    print("ğŸ“Š çˆ¬è™«ç³»ç»ŸçŠ¶æ€")
    print("=" * 40)
    
    # æ£€æŸ¥æ–‡ä»¶
    files_to_check = [
        "crawler/simple_crawler.py",
        "crawler/config.py", 
        "index.html",
        "style.css"
    ]
    
    for file_path in files_to_check:
        if os.path.exists(file_path):
            print(f"âœ… {file_path}")
        else:
            print(f"âŒ {file_path}")
            
    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_dir = "crawler/data"
    if os.path.exists(data_dir):
        files = os.listdir(data_dir)
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {len(files)} ä¸ª")
        
        # æ˜¾ç¤ºæœ€æ–°çš„å‡ ä¸ªæ–‡ä»¶
        json_files = [f for f in files if f.endswith('.json')]
        if json_files:
            latest_json = max(json_files, key=lambda f: os.path.getctime(os.path.join(data_dir, f)))
            print(f"ğŸ“„ æœ€æ–°æ•°æ®: {latest_json}")
    else:
        print("ğŸ“ æ•°æ®ç›®å½•ä¸å­˜åœ¨")

def main():
    parser = argparse.ArgumentParser(description='è‹±å›½ä¼˜æƒ çˆ¬è™«ç®¡ç†å·¥å…·')
    parser.add_argument('action', choices=[
        'run', 'schedule', 'update', 'install', 'status'
    ], help='è¦æ‰§è¡Œçš„æ“ä½œ')
    
    args = parser.parse_args()
    
    print("ğŸš€ è‹±å›½ä¼˜æƒ çˆ¬è™«ç®¡ç†å·¥å…·")
    print("=" * 40)
    
    if args.action == 'run':
        print("ğŸ”„ ç«‹å³è¿è¡Œçˆ¬è™«...")
        success = run_simple_crawler()
        if success:
            update_website()
            
    elif args.action == 'schedule':
        start_scheduler()
        
    elif args.action == 'update':
        print("ğŸ”„ æ›´æ–°ç½‘ç«™å†…å®¹...")
        update_website()
        
    elif args.action == 'install':
        install_dependencies()
        
    elif args.action == 'status':
        show_status()

if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºäº¤äº’å¼èœå•
    if len(sys.argv) == 1:
        print("ğŸš€ è‹±å›½ä¼˜æƒ çˆ¬è™«ç®¡ç†å·¥å…·")
        print("=" * 40)
        print("1. ç«‹å³è¿è¡Œçˆ¬è™«")
        print("2. å¯åŠ¨å®šæ—¶è°ƒåº¦å™¨")
        print("3. æ›´æ–°ç½‘ç«™å†…å®¹")
        print("4. å®‰è£…ä¾èµ–")
        print("5. æŸ¥çœ‹çŠ¶æ€")
        print("6. é€€å‡º")
        
        while True:
            try:
                choice = input("\\nè¯·é€‰æ‹©æ“ä½œ (1-6): ").strip()
                
                if choice == '1':
                    run_simple_crawler()
                    update_website()
                elif choice == '2':
                    start_scheduler()
                elif choice == '3':
                    update_website()
                elif choice == '4':
                    install_dependencies()
                elif choice == '5':
                    show_status()
                elif choice == '6':
                    print("ğŸ‘‹ å†è§!")
                    break
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")
                    
            except KeyboardInterrupt:
                print("\\nğŸ‘‹ å†è§!")
                break
    else:
        main()
